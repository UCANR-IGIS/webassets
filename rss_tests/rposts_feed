<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	
	xmlns:georss="http://www.georss.org/georss"
	xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#"
	>

<channel>
	<title>Richard Plant &#8211; R-posts.com</title>
	<atom:link href="https://r-posts.com/author/replant/feed/" rel="self" type="application/rss+xml" />
	<link>https://r-posts.com</link>
	<description>Guest R posts to go on R-bloggers.com</description>
	<lastBuildDate>Wed, 14 Oct 2020 14:55:23 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.5.5</generator>
	<item>
		<title>Spatial Data Analysis Using Artificial Neural Networks, Part 2</title>
		<link>https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/</link>
					<comments>https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/#respond</comments>
		
		<dc:creator><![CDATA[Richard Plant]]></dc:creator>
		<pubDate>Wed, 14 Oct 2020 14:55:23 +0000</pubDate>
				<category><![CDATA[R]]></category>
		<guid isPermaLink="false">http://r-posts.com/?p=2078</guid>

					<description><![CDATA[Spatial Data Analysis Using Artificial Neural Networks, Part 2 Richard E. Plant Additional topic to accompany Spatial Data Analysis in Ecology and Agriculture using R, Second Edition http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm Chapter and section references are contained in that text, which is referred to as SDA2. Data sets and R code are available in the Additional Topics through &#8230; <a href="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/" class="more-link">Continue reading <span class="screen-reader-text">Spatial Data Analysis Using Artificial Neural Networks, Part 2</span></a><hr style="border-top:black solid 1px" /><a href="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/">Spatial Data Analysis Using Artificial Neural Networks, Part 2</a> was first posted on October 14, 2020 at 2:55 pm.<br />]]></description>
										<content:encoded><![CDATA[<h1>Spatial Data Analysis Using Artificial Neural Networks, Part 2</h1>

Richard E. Plant</p>

Additional topic to accompany <em>Spatial Data Analysis in Ecology and Agriculture using R, Second Edition</em>

<a href="http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm">http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm</a>

Chapter and section references are contained in that text, which is referred to as SDA2. Data sets and R code are available in the Additional Topics through the link above.

<h2>Forward</h2>

This post originally appeared as an Additional Topic to my book as described above. Because it may be of interest to a wider community than just agricultural scientists and ecologists, I am posting it here as well. However, it is too long to be a single post. I have therefore divided it into two parts, posted successively. I have also removed some of the mathematical derivations. This is the second of these two posts. In the first post we discussed ANNs with a single hidden layer and constructed one to see how it functions. This second post discusses multilayer and radial basis function ANNs,  ANNs for multiclass problems, and the determination of predictor variable importance.  Some of the graphics and formulas became somewhat blurred when I transfered them from the original document to this post. I apologize for that, and again, if you want to see the clearer version you should go to the website listed above and download the pdf file.

<h2>3. The Multiple Hidden Layer ANN</h2>

Although artificial neural networks with a single hidden layer can provide quite general solution capabilities (Venables and Ripley, 2002, p. 245), preference is sometimes expressed for multiple hidden layers. The most widely used R ANN package providing multiple hidden layer capability is <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet </span>(Fritsch et al., 2019). Let’s try it out on our training data set using two hidden layers with four cells each (Fig. 19).<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; mod.neuralnet &lt;- neuralnet(QUDO ~ MAT +<br />
+     Precip,</span><span style="font-size: 12pt;font-family: courier new, courier, monospace"> data = Set2.Train,<br />
+     hidden = c(4, 4), act.fct = &#8220;logistic&#8221;,</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+     linear.output = FALSE)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; Predict.neuralnet &lt;- <br />
+     predict(mod.neuralnet, Set2.Test)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; plotnet(mod.neuralnet, cex = 0.75) <em># Fig. 19a<br />
</em>&gt; p &lt;- plot.ANN(Set2.Test,<br />
+  Predict.neuralnet, 0.5,<br />
+  </span><span style="font-size: 12pt;font-family: courier new, courier, monospace">&#8220;neuralnet Predictions, Two Hidden Layers&#8221;) </span>

<br />
<br />
The argument <span style="font-size: 12pt;font-family: courier new, courier, monospace">hidden = c(4, 4)</span> specifies two hidden layers of four cells each, and the argument <span style="font-size: 12pt;font-family: courier new, courier, monospace">linear.output = FALSE</span> specifies that this is a classification and not a regression problem. As with the single layer ANNs of the previous section, the two layer ANNs of <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet()</span> produce a variety of outputs, some tame and some not so tame (Exercise 6).<br />
<br />
<br />


<img data-attachment-id="2085" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-19a/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19a.jpg?fit=319%2C198&amp;ssl=1" data-orig-size="319,198" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 19a" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19a.jpg?fit=319%2C198&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19a.jpg?fit=319%2C198&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19a.jpg?resize=234%2C145" alt="" class="alignnone  wp-image-2085" width="234" height="145" data-recalc-dims="1" /> <img data-attachment-id="2086" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-19b/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19b.jpg?fit=201%2C201&amp;ssl=1" data-orig-size="201,201" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 19b" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19b.jpg?fit=201%2C201&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19b.jpg?fit=201%2C201&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19b.jpg?resize=201%2C201" alt="" class="alignnone size-full wp-image-2086" width="201" height="201" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19b.jpg?w=201&amp;ssl=1 201w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-19b.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 201px) 100vw, 201px" data-recalc-dims="1" />

<br />
Figure 19. (a) Diagram of the <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet </span>ANN with two hidden layers; (b) prediction map of the ANN.<br />
<br />


&nbsp;

<br />
Instead of using Newton’s method found in the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">nlm()</span> and approximated in <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet()</span>,  it has become common in ANN work to use an alternative method called <em>gradient descent</em>. The <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet</span> package permits the selection of various versions of the gradient descent algorithm. Suppose for definiteness that the objective function <em>J</em> is the sum of squared errors (everything works the same if it is, for example, the cross entropy, but the discussion is more complex). Denote the combined sets of biases and weights by <em>w<sub>i</sub></em>, 1 = 1,…,<em>n<sub>w</sub></em>,  (for our example of Section 2, with one hidden layer and <em>M</em> = 4, <em>n<sub>w</sub></em> = 17). If the ANN returns an estimate <img data-attachment-id="2147" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/yhat-2/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Yhat-1.jpg?fit=14%2C17&amp;ssl=1" data-orig-size="14,17" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Yhat" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Yhat-1.jpg?fit=14%2C17&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Yhat-1.jpg?fit=14%2C17&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Yhat-1.jpg?resize=14%2C17" alt="" class="alignnone size-full wp-image-2147" width="14" height="17" data-recalc-dims="1" /> of the vector <em>Y </em>of class labels then we can then write the sum of squared errors of the ANN as

<table style="width: 85.7143%">
<tbody>
<tr>
<td style="width: 75.7685%" width="554">
<p style="text-align: center"><img data-attachment-id="2090" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eq-7/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-7.jpg?fit=248%2C49&amp;ssl=1" data-orig-size="248,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eq 7" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-7.jpg?fit=248%2C49&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-7.jpg?fit=248%2C49&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-7.jpg?resize=248%2C49" alt="" class="alignnone size-full wp-image-2090" width="248" height="49" data-recalc-dims="1" />.</p>
</td>
<td style="width: 9.76489%" width="70">
(7)
</td>
</tr>
</tbody>
</table>

The intent of this notation is to indicate explicitly that the vector of estimates  depends on the biases and weights. Minimizing  <em>J(w<sub>1</sub>,…,w<sub>nw</sub>)</em> is the unconstrained nonlinear minimization problem to which the gradient descent method is applied. The set of values of <em>J(w</em><sub>1</sub>,…,<em>w<sub>nw</sub></em>) describes a surface (technically a “hypersurface”) sitting “above” the <em>n<sub>w</sub></em> dimensional space of the weights and biases. If you have trouble visualizing this, it might help to imagine the case in which <em>n<sub>w</sub></em> = 2, in which case the surface of values of <em>J(w</em><sub>1</sub>,<em>w</em><sub>2</sub>)  lies above the plane of values of (<em>w</em><sub>1</sub>,<em>w</em><sub>2</sub>)<em><sub>.</sub></em> The <a href="https://en.wikipedia.org/wiki/Gradient_descent">Wikipedia article</a> discussing the gradient descent method is truly excellent, and I will not try to improve on it. From this article we can see that in order to find the minimum of the objective function <em>J</em>, starting from a particular point in the  space we move in the direction of the gradient

<table style="width: 85.3526%">
<tbody>
<tr>
<td style="width: 73.0561%" width="554">
<p style="text-align: center"><img data-attachment-id="2091" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eq-8/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-8.jpg?fit=227%2C49&amp;ssl=1" data-orig-size="227,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eq 8" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-8.jpg?fit=227%2C49&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-8.jpg?fit=227%2C49&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-8.jpg?resize=227%2C49" alt="" class="alignnone size-full wp-image-2091" width="227" height="49" data-recalc-dims="1" />.</p>
</td>
<td style="width: 12.1157%" width="70">
(8)
</td>
</tr>
</tbody>
</table>

Here the <em>e<sub>i</sub></em> are unit vectors in the space of the weights and biases, and the gradient points in the direction of steepest descent (and of steepest ascent – which would take us in the wrong direction) of <em>J</em>. As described in the Wikipedia article, the algorithm moves in a sequence of steps

<table style="width: 81.9168%">
<tbody>
<tr>
<td style="width: 71.7902%" width="554">
<p style="text-align: center"><img data-attachment-id="2092" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eq-9/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-9.jpg?fit=159%2C28&amp;ssl=1" data-orig-size="159,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eq 9" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-9.jpg?fit=159%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-9.jpg?fit=159%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-9.jpg?resize=159%2C28" alt="" class="alignnone size-full wp-image-2092" width="159" height="28" data-recalc-dims="1" />.</p>
</td>
<td style="width: 9.94572%" width="70">
(9)
</td>
</tr>
</tbody>
</table>

Here <em>w</em><sub>(<em>k</em>) </sub>is the estimate of the vector <em>w</em> of biases and weights at the <em>k<sup>th</sup></em> gradient descent step, and <img data-attachment-id="2093" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/gamma/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?fit=15%2C22&amp;ssl=1" data-orig-size="15,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gamma" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?fit=15%2C22&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?fit=15%2C22&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?resize=15%2C22" alt="" class="alignnone size-full wp-image-2093" width="15" height="22" data-recalc-dims="1" /> is the length of the step taken in the direction of the negative gradient (sometimes called the <em>learning rate</em>).

All of the partial derivatives in Equation (8) must be estimated numerically, so it is evident that although the problem is much smaller than that involving the computation of the Hessian matrix of second partial derivatives, it still involves a lot of computation. For that reason, ANN algorithms often use an approximate form of gradient descent. The most common of these is <em>stochastic gradient descent</em>. In this method, instead of estimating all <em>n<sub>w</sub></em> partial derivatives at each step, a random subset of a fixed size (possibly as small as one) is chosen and the direction is determined based on minimizing this subset. If you have read the Wikipedia article above then this <a href="https://deepai.org/machine-learning-glossary-and-terms/stochastic-gradient-descent">short description</a> provides a good explanation of stochastic gradient descent, and <a href="https://gluon.mxnet.io/chapter06_optimization/gd-sgd-scratch.html">this discussion</a> provides more detail. In particular, you can see from these articles that it is even possible that the stochastic gradient descent method can actually move past a local minimum and find the global minimum.

In fact, the gradient in Equation (9) is not actually computed directly. Instead, an algorithm called <em>back propagation </em>is generally used to estimate the partial derivatives. The Additional Topic on which this post is based includes a mathematical description of back propagation, but I have decided to remove that from this post, both to bring the post to a reasonable length and to reduce the mathematical complexity. The basic idea of back propagation is to compute the partial derivatives in Equation (8) by repeated applications of the chain rule of differential calculus. You can see the full derivation by downloading the Additional Topic.

The <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet()</span> function has available several forms of back propagation. As a default it uses the <em>resilient propagation </em>algorithm (Riedmiller et al., 1993). This algorithm recognizes that the size of the step from <em>w</em><sub>(<em>k</em>) </sub> to <em>w</em><sub>(<em>k+1</em>) </sub> depends not only on the size of <em><img data-attachment-id="2093" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/gamma/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?fit=15%2C22&amp;ssl=1" data-orig-size="15,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="gamma" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?fit=15%2C22&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?fit=15%2C22&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/gamma.jpg?resize=15%2C22" alt="" class="alignnone size-full wp-image-2093" width="15" height="22" data-recalc-dims="1" /></em> but also on the sizes of the components of <img data-attachment-id="2095" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/del-j/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?fit=28%2C22&amp;ssl=1" data-orig-size="28,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="del J" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?fit=28%2C22&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?fit=28%2C22&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?resize=28%2C22" alt="" class="alignnone size-full wp-image-2095" width="28" height="22" data-recalc-dims="1" />, and that this latter dependence may be disadvantageous. For this reason resilient propagation develops a step size that is less dependent on the magnitudes of the components of <img data-attachment-id="2095" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/del-j/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?fit=28%2C22&amp;ssl=1" data-orig-size="28,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="del J" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?fit=28%2C22&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?fit=28%2C22&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/del-J.jpg?resize=28%2C22" alt="" class="alignnone size-full wp-image-2095" width="28" height="22" data-recalc-dims="1" />, using these values primarily to determine the direction of the step. There are other methods available and you can see the <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet </span>package documentation for details. The package also makes available the cross-entropy error function in addition to the default error sum of squares.

Now that we have an idea of how the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet()</span> works, let’s try it out on the full <em>QUDO</em> data set. We will use two hidden layers of four cells each, similar to Fig 19a. When I worked with on this problem it turned out that <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet()</span> failed to converge in several attempts when applied to the full data set. The largest training set for which convergence was obtained had <em>n</em> = 200 data records evenly divided between <em>QUDO =</em> 0 and <em>QUDO</em> = 1. Fig. 20a shows the result of applying <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet()</span> to this training data set using a cutoff of 0.73, which was determined from a ROC curve analysis (code not shown). The error rate is 0.225, which is not as good as that obtained in Section 2 using <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet()</span>.<br />
<br />
<br />
                   <img data-attachment-id="2099" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-20a/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20a.jpg?fit=201%2C201&amp;ssl=1" data-orig-size="201,201" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 20a" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20a.jpg?fit=201%2C201&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20a.jpg?fit=201%2C201&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20a.jpg?resize=154%2C154" alt="" class="alignnone  wp-image-2099" width="154" height="154" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20a.jpg?w=201&amp;ssl=1 201w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20a.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 154px) 100vw, 154px" data-recalc-dims="1" />         <img data-attachment-id="2100" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-20b/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20b.jpg?fit=269%2C269&amp;ssl=1" data-orig-size="269,269" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 20b" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20b.jpg?fit=269%2C269&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20b.jpg?fit=269%2C269&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20b.jpg?resize=167%2C167" alt="" class="alignnone  wp-image-2100" width="167" height="167" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20b.jpg?w=269&amp;ssl=1 269w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-20b.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 167px) 100vw, 167px" data-recalc-dims="1" /><br />
                                       (a)                                                      (b)

<br />
Figure 20. (a) Plot of prediction regions of <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet()</span> with two hidden layers and a training data set of 200 randomly selected records; (b) application of the resulting model to the full set.<br />
<br />


&nbsp;

Sections 2 and 3 have given us a general idea of how a multilayer perceptron works. Although the MLP is probably the most commonly used ANN for the type of classification problem discussed here, several alternatives exist. The next section discusses one of them, the radial basis function ANN.

<h2>4. The Radial Basis Function ANN</h2>

The seemingly obscure term <em>radial basis function</em> ANN is actually not hard to parse out. We will start with word <em>basis</em>. As you may remember from linear algebra, a <em>basis</em> is a set of vectors in the vector space that have the property that any vector in the vector space can be described as a linear combination of elements of this set. For discussion and a picture see <a href="https://en.wikipedia.org/wiki/Basis_(linear_algebra)">here</a>. If you had a course in <a href="https://en.wikipedia.org/wiki/Fourier_series">Fourier series</a>, you may remember that the Fourier series functions form a basis for a set of functions on an infinite dimensional vector space. <a href="https://en.wikipedia.org/wiki/Radial_basis_function">Radial basis functions</a> do the same sort of thing. A radial function is a function of the form

<table style="width: 83.906%">
<tbody>
<tr>
<td style="width: 71.9711%" width="554">
                  <img data-attachment-id="2101" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eq-20/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-20.jpg?fit=111%2C28&amp;ssl=1" data-orig-size="111,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eq 20" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-20.jpg?fit=111%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-20.jpg?fit=111%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-20.jpg?resize=139%2C35" alt="" class="alignnone  wp-image-2101" width="139" height="35" data-recalc-dims="1" />,
</td>
<td style="width: 11.9349%" width="70">
(10)
</td>
</tr>
</tbody>
</table>

where <em>c</em> is the center point of a vector space. In other words, a radial function is radially symmetric and depends only on the distance of the vector <em>x</em> from the center. Again we are using the terms “point” and “vector” synonymously. The radial basis function commonly used in ANNs is the Gaussian function (i.e., the <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a>). When <em>x</em> is a two dimensional quantity, as it is in our case (<em>MAT</em> and <em>Precip</em>), the function <img data-attachment-id="2117" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/phix/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?fit=35%2C22&amp;ssl=1" data-orig-size="35,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phi(x)" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?fit=35%2C22&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?fit=35%2C22&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?resize=35%2C22" alt="" class="alignnone size-full wp-image-2117" width="35" height="22" data-recalc-dims="1" /> can be visualized, and a picture of it is shown <a href="https://online.stat.psu.edu/stat505/lesson/4/4.2">here</a>. Thus, as with a Fourier series, a radial basis function forms a basis with which to approximate other functions – in particular, in our case, functions associated with the process of classification via an ANN.

        <img data-attachment-id="2102" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-21/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-21.jpg?fit=624%2C345&amp;ssl=1" data-orig-size="624,345" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 21" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-21.jpg?fit=450%2C249&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-21.jpg?fit=450%2C249&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-21.jpg?resize=450%2C249" alt="" class="alignnone size-medium wp-image-2102" width="450" height="249" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-21.jpg?resize=450%2C249&amp;ssl=1 450w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-21.jpg?w=624&amp;ssl=1 624w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />

<br />
Figure 21. Architecture of the radial basis function ANN.<br />
<br />


&nbsp;

The way a radial basis function (RBF) works is as follows. Suppose again that the cost function <em>J</em> is based on the sum of squared errors. Referring to Fig. 21, which shows a schematic of a radial basis function ANN with a single hidden layer, the input activations for the <em>i<sup>th </sup></em>data record are, as with the multilayer perceptron, simply the values <em>X<sub>i,j</sub></em>. We will describe the combined output of all of the hidden cells in terms of the basis function <img data-attachment-id="2117" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/phix/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?fit=35%2C22&amp;ssl=1" data-orig-size="35,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phi(x)" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?fit=35%2C22&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?fit=35%2C22&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/phix.jpg?resize=35%2C22" alt="" class="alignnone size-full wp-image-2117" width="35" height="22" data-recalc-dims="1" />. Specifically, for a fixed <em>i</em> define a function <em>f(X)</em> by

<table style="width: 83.5443%">
<tbody>
<tr>
<td style="width: 71.6094%;text-align: center" width="554"><img data-attachment-id="2118" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eq-21/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-21.jpg?fit=180%2C49&amp;ssl=1" data-orig-size="180,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eq 21" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-21.jpg?fit=180%2C49&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-21.jpg?fit=180%2C49&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-21.jpg?resize=180%2C49" alt="" class="alignnone size-full wp-image-2118" width="180" height="49" data-recalc-dims="1" />.</td>
<td style="width: 11.7541%" width="70">
(11)
</td>
</tr>
</tbody>
</table>

Here <em>c<sub>m</sub></em> is the center of the <em>m<sup>th</sup></em> radial basis function and <em>j<sub>m</sub></em> measures the size of the contribution of this component to the overall function . If our objective function is the sum of squared errors then we plug Equation (11) into the definition of <em>J</em> to get

<table style="width: 84.4485%">
<tbody>
<tr>
<td style="width: 70.7052%;text-align: center" width="554"><img data-attachment-id="2119" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eq-22/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-22.jpg?fit=193%2C97&amp;ssl=1" data-orig-size="193,97" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eq 22" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-22.jpg?fit=193%2C97&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-22.jpg?fit=193%2C97&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-22.jpg?resize=193%2C97" alt="" class="alignnone size-full wp-image-2119" width="193" height="97" data-recalc-dims="1" />.</td>
<td style="width: 13.5624%" width="70">
(12)
</td>
</tr>
</tbody>
</table>

Each of the <em>M</em> elements of the sum <img data-attachment-id="2120" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/sigmam/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/sigmam.jpg?fit=118%2C28&amp;ssl=1" data-orig-size="118,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="sigmam" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/sigmam.jpg?fit=118%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/sigmam.jpg?fit=118%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/sigmam.jpg?resize=118%2C28" alt="" class="alignnone size-full wp-image-2120" width="118" height="28" data-recalc-dims="1" /> forms one of the cells in the hidden layer of the ANN (Fig. 19) (note that the order of the summation has been reversed from Section 2 so that first the summation over <em>i</em> takes place first in each cell and then second over the <em>m</em> hidden cells). The links between the hidden cells and the output cell hold the weights <em>j<sub>m</sub></em>. The weighted outputs of the <em>m</em> cells, given by <img data-attachment-id="2121" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/am2/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/am2.jpg?fit=152%2C28&amp;ssl=1" data-orig-size="152,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="am2" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/am2.jpg?fit=152%2C28&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/am2.jpg?fit=152%2C28&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/am2.jpg?resize=152%2C28" alt="" class="alignnone size-full wp-image-2121" width="152" height="28" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/am2.jpg?w=152&amp;ssl=1 152w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/am2.jpg?resize=150%2C28&amp;ssl=1 150w" sizes="(max-width: 152px) 100vw, 152px" data-recalc-dims="1" />, are summed to determine the activation <em>a<sup>(3)</sup></em><sup><em> </em></sup>of the output cell.

The architecture described here and shown in Fig. 21 is the simplest form of a radial basis function ANN. More bells and whistles can be added; for example, biases can be incorporated as constant terms in the sums of Equation (11). In addition, a logistic or other function can be interposed between the output cell level internal value  and the activation <em>a<sup>(3)</sup></em>, similar to that of the MLP (Fig. 8).

<br />
The first step in the RBF computational process is to determine a set of centers <em>c<sub>m</sub></em> in Equation (11). The determination is made by trying to place the centers so that their location matches as closely as possible the distribution in the data space of values <img data-attachment-id="2148" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/x-pair/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/X-pair.jpg?fit=70%2C28&amp;ssl=1" data-orig-size="70,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="X pair" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/X-pair.jpg?fit=70%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/X-pair.jpg?fit=70%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/X-pair.jpg?resize=70%2C28" alt="" class="alignnone size-full wp-image-2148" width="70" height="28" data-recalc-dims="1" />. Some RBF systems do this by carrying out a <em>k</em>-means clustering (SDA2 Sec. 12.6.1) of the space of data records, where the number of means <em>k</em> equals the number <em>M</em> of hidden cells (e.g. Nunes da Silva et al., 2017, p.121). Another possibility is to carry out a <a href="https://en.wikipedia.org/wiki/Self-organizing_map"><em>Kohonen training process</em></a>. This is basically an unsupervised clustering algorithm analogous to <em>k</em>-means clustering, although, as is often the case with methods related to ANNs, the terminology is a bit more pretentious: in this case the result is called a “self-organizing map.” We will be using the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">rbf()</span> of the RSNNS package (Bergmeir and Benitez, 2012). It does not use <em>k-</em>means clustering, but the use of the Kohonen training process is available as an option. The default method, which in my experience works the best, is called the “rbf weights” method. Fundamentally, the rbf weights method is, in the words of the creators of the package (Zell et al, 1998, p. 177), “rather simple.” Essentially, the weights are distributed in the data space to match a random sample of the values of the data records. There are a few adjustments that can be used, but this description captures the basics. Once the values of the centers <em>c<sub>m</sub></em> are established the final step of the training process is to determine the values of the link weights <em>j<sub>m</sub></em><sub> </sub>along with the values of the biases if there are any. This is accomplished in <span style="font-size: 12pt;font-family: courier new, courier, monospace">rbf()</span> by gradient descent. Similarly to the multilayer perceptron ANNs described in Section 3, the gradient descent algorithm carries out an iterative process in which the link weights are adjusted by a series of small steps of the form

<table style="width: 84.6293%">
<tbody>
<tr>
<td style="width: 72.6944%" width="554">
<p style="text-align: center"><img data-attachment-id="2122" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eq-23/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-23.jpg?fit=90%2C49&amp;ssl=1" data-orig-size="90,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eq 23" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-23.jpg?fit=90%2C49&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-23.jpg?fit=90%2C49&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/eq-23.jpg?resize=90%2C49" alt="" class="alignnone size-full wp-image-2122" width="90" height="49" data-recalc-dims="1" />,</p>
</td>
<td style="width: 11.9349%" width="70">
(13)
</td>
</tr>
</tbody>
</table>

where <img data-attachment-id="2123" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/eta/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eta.jpg?fit=24%2C33&amp;ssl=1" data-orig-size="24,33" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="eta" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eta.jpg?fit=24%2C33&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eta.jpg?fit=24%2C33&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/eta.jpg?resize=19%2C27" alt="" class="alignnone  wp-image-2123" width="19" height="27" data-recalc-dims="1" />  is a small constant.

This brief introduction is sufficient to allow us to try out an RBF ANN. The package <span style="font-size: 12pt;font-family: courier new, courier, monospace">RSNNS</span> is a port to R of the Stuttgart Neural Network Simulator (Zell et al, 1998), or SNNS, which contains implementations of many ANNs. As a segue to the use of the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">rbf()</span>, which creates a radial basis function ANN, we will first test the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">mlp()</span> from the same package. This provides a multilayer perceptron solution that can be compared with those of Sections 2 and 3. The use of functions with the SNNS package is slightly different from what we have seen in Sections 2 and 3. Working initially with the 50 record Training data set, we first prepare its data records to serve as arguments for the ANN functions.<br />
<br />


<span style="font-size: 10pt;font-family: courier new, courier, monospace">library(RSNNS) </span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">Train.Values &lt;- Set2.Train[,c(&#8220;MAT&#8221;, &#8220;Precip&#8221;)]</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">Train.Targets &lt;- <br />
+   decodeClassLabels(as.character(Set2.Train$QUDO))</span>

&nbsp;

<br />
<br />
We will start by testing the <span style="font-size: 12pt;font-family: courier new, courier, monospace">RSNNS</span> multilayer perceptron using a single hidden layer with four cells. The package has a very nice function called <span style="font-size: 12pt;font-family: courier new, courier, monospace">plotIterativeError()</span> that permits one to visualize how the error declines as the iterations proceed. We will first run the ANN and check out the output. These are slight differences in how the polymorphic R functions are implemented from those of the previous sections.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; mod.SNNS &lt;- mlp(Train.Values, Train.Targets,<br />
+    size = 4)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; Predict.SNNS &lt;- predict(mod.SNNS,<br />
+     Set2.Test[,1:2])</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; head(Predict.SNNS)</span><br />
<strong><span style="font-size: 12pt;font-family: courier new, courier, monospace">          [,1]      [,2]</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[1,] 0.6385500 0.3543552</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[2,] 0.6247426 0.3684126</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[3,] 0.6107571 0.3826691</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[4,] 0.5966275 0.3970889</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[5,] 0.5823891 0.4116347</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[6,] 0.5680778 0.4262682</span></strong>

&nbsp;

<br />
<br />
For this binary classification problem the output of the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">predict()</span> is in the form of a matrix whose columns sum to 1, reflecting the alternative predictions. Using this information, we can plot the prediction region (Fig. 22a).

<p style="text-align: center"><img data-attachment-id="2124" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-22a/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22a.jpg?fit=205%2C205&amp;ssl=1" data-orig-size="205,205" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 22a" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22a.jpg?fit=205%2C205&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22a.jpg?fit=205%2C205&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22a.jpg?resize=205%2C205" alt="" class="alignnone size-full wp-image-2124" width="205" height="205" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22a.jpg?w=205&amp;ssl=1 205w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22a.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 205px) 100vw, 205px" data-recalc-dims="1" />    <img data-attachment-id="2125" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-22b/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22b.jpg?fit=207%2C207&amp;ssl=1" data-orig-size="207,207" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 22b" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22b.jpg?fit=207%2C207&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22b.jpg?fit=207%2C207&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22b.jpg?resize=207%2C207" alt="" class="alignnone size-full wp-image-2125" width="207" height="207" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22b.jpg?w=207&amp;ssl=1 207w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-22b.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 207px) 100vw, 207px" data-recalc-dims="1" /></p>

Figure 22. Plots of (a) the predicted regions and (b) the iterative error with the default iteration limit.<br />
<br />


&nbsp;

Next we plot the iterative error (Fig. 22b).<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace"><br />
&gt; plotIterativeError(mod.SNNS) <em># Fig. 22b</em></span>

&nbsp;

<br />
<br />
The error clearly declines, and it may or may not be leveling off. To check, we will set the argument <span style="font-size: 12pt;font-family: courier new, courier, monospace">maxit </span>to a very large value and see what happens.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; mod.SNNS &lt;- mlp(Train.Values, Train.Targets,<br />
+    size = 4, </span><span style="font-size: 12pt;font-family: courier new, courier, monospace">maxit = 50000)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; stop_time &lt;- Sys.time()</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; stop_time &#8211; start_time</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>Time difference of 5.016404 secs<br />
</strong>&gt; Predict.SNNS &lt;- predict(mod.SNNS,<br />
+    Set2.Test[,1:2])</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; p &lt;- plot.ANN(Set2.Test, Predict.SNNS[,2],<br />
+    0.5,</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+    &#8220;RSNNS mlp Prediction, 50,000 Iterations&#8221;) <em><br />
</em>&gt; plotIterativeError(mod.SNNS) <em># Fig. 23b</em></span>

<br />
              <img data-attachment-id="2126" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-23a/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23a.jpg?fit=195%2C195&amp;ssl=1" data-orig-size="195,195" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 23a" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23a.jpg?fit=195%2C195&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23a.jpg?fit=195%2C195&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23a.jpg?resize=164%2C164" alt="" class="alignnone  wp-image-2126" width="164" height="164" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23a.jpg?w=195&amp;ssl=1 195w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23a.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 164px) 100vw, 164px" data-recalc-dims="1" />       <img data-attachment-id="2127" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-23b/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23b.jpg?fit=195%2C195&amp;ssl=1" data-orig-size="195,195" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 23b" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23b.jpg?fit=195%2C195&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23b.jpg?fit=195%2C195&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23b.jpg?resize=176%2C176" alt="" class="alignnone  wp-image-2127" width="176" height="176" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23b.jpg?w=195&amp;ssl=1 195w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-23b.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 176px) 100vw, 176px" data-recalc-dims="1" /><br />
                                  (a)                                            (b)<br />
Figure 23. Plots of (a) the predicted regions and (b) the iterative error with an iteration limit of 50,000.<br />
<br />


&nbsp;

The iteration error clearly declines some more. The prediction region does not look like most of those that we have seen earlier, and does not look very intuitive, but of course the machine doesn’t know that. The time to run 50,000 iterations for this problem is not bad at all.

<br />
Now we can move on to the radial basis function ANN, implemented via the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">rbf()</span>. First we will try it with all arguments set to their default values (Fig. 24).<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; mod.SNNS &lt;- rbf(Train.Values, Train.Targets)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; Predict.SNNS &lt;- predict(mod.SNNS, Set2.Test)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; p &lt;- plot.ANN(Set2.Test, Predict.SNNS[,2], <br />
+   0.5,</span><span style="font-size: 12pt;font-family: courier new, courier, monospace"> &#8220;RSNNS rbf Prediction,<br />
+   Default Values&#8221;) <br />
<br />
</span>

<img data-attachment-id="2128" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-24/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-24.jpg?fit=174%2C174&amp;ssl=1" data-orig-size="174,174" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 24" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-24.jpg?fit=174%2C174&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-24.jpg?fit=174%2C174&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-24.jpg?resize=174%2C174" alt="" class="alignnone size-full wp-image-2128 aligncenter" width="174" height="174" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-24.jpg?w=174&amp;ssl=1 174w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-24.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 174px) 100vw, 174px" data-recalc-dims="1" />

Figure 24. Prediction regions for the model developed using <span style="font-size: 12pt;font-family: courier new, courier, monospace">rbf()</span> with default values.<br />
<br />


&nbsp;

<br />
The “radial” part of the term radial basis function becomes very apparent! A glance at the documentation via <span style="font-size: 12pt;font-family: courier new, courier, monospace">?rbf</span> reveals that the default value for the number of hidden cells is 5. Many sources discussing radial basis function ANNs point out that a large number of hidden cells is often necessary to correctly model the data. We will try another run with the number of hidden cells increased to 40.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; mod.SNNS &lt;- rbf(Train.Values, Train.Targets,<br />
+    size = 40)</span>

&nbsp;

<br />
The rest of the code is identical to what we have seen before and is not shown. The prediction region is similar to that of Fig. 25a below and is not shown. The error rate is about what we have seen from the other methods.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; length(which(Set2.Pred$TrueVal !=<br />
+   Set2.Pred$PredVal)) /</span><span style="font-size: 12pt;font-family: courier new, courier, monospace"> nrow(Set2.Train) </span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 0.22</strong></span>

<br />
                         <img data-attachment-id="2129" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-26a/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-26a.jpg?fit=157%2C157&amp;ssl=1" data-orig-size="157,157" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 26a" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-26a.jpg?fit=157%2C157&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-26a.jpg?fit=157%2C157&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-26a.jpg?resize=157%2C157" alt="" class="alignnone size-full wp-image-2129" width="157" height="157" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-26a.jpg?w=157&amp;ssl=1 157w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-26a.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 157px) 100vw, 157px" data-recalc-dims="1" />       <img data-attachment-id="2130" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-26b/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-26b.jpg?fit=156%2C156&amp;ssl=1" data-orig-size="156,156" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 26b" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-26b.jpg?fit=156%2C156&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-26b.jpg?fit=156%2C156&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-26b.jpg?resize=156%2C156" alt="" class="alignnone size-full wp-image-2130" width="156" height="156" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-26b.jpg?w=156&amp;ssl=1 156w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-26b.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 156px) 100vw, 156px" data-recalc-dims="1" /><br />
                                       (a)                                      (b)

<br />
Figure 25. (a) Prediction regions and (b) error estimate for the model developed using <span style="font-size: 12pt;font-family: courier new, courier, monospace">rbf()</span> with the full data set.<br />
<br />


&nbsp;

The performance of <span style="font-size: 12pt;font-family: courier new, courier, monospace">rbf()</span> on the full data set is not particularly spectacular.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; length(which(Set2.Pred$TrueVal != <br />
+   Set2.Pred$PredVal)) /</span><span style="font-size: 12pt;font-family: courier new, courier, monospace"> nrow(data.Set2U) </span><br />
<strong><span style="font-size: 12pt;font-family: courier new, courier, monospace">[1] 0.2324658</span></strong>

&nbsp;

<br />
<br />
Now that we have had a chance to compare several ANNs on a binary selection problem, an inescapable conclusion is that, for the problem we are studying at least, the old workhorse <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet()</span> acquits itself pretty well. In Section 5 we will try out various ANNs on a multi-category classification process. In this case each of the ANNs we are considering has an output cell for each class label.

&nbsp;

<h2>5. A Multicategory Classification Problem</h2>

The multicategory classification problem that we will study in this chapter is the same one that we studied in the <a href="https://psfaculty.plantsciences.ucdavis.edu/plant/method%20comparison.pdf">Additional Topic on Comparison of Supervised Classification Methods</a>. A detailed introduction is given there. In brief, the data set is the augmented Data Set 2 from SDA2. It is based on the Wieslander survey of oak species in California (Wieslander, 1935) and contains records of the presence or absence of blue oak (<em>Quercus douglasii, QUDO</em>), coast live oak (<em>Quercus agrifolia, QUAG</em>), canyon oak (<em>Quercus chrysolepis, QUCH</em>), black oak (<em>Quercus kelloggii, QUKE</em>), valley oak (<em>Quercus lobata, QULO</em>), and interior live oak (<em>Quercus wislizeni, QUWI</em>). As implemented it is the data frame <span style="font-size: 12pt;font-family: courier new, courier, monospace">data.Set2U</span>; until now we have grouped all of the non-QUDO species together and assigned them the value <em>QUDO</em> = 0. In this section we will create the class label species and assign the appropriate character value to each record.<br />
<br />


<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; species &lt;- character(nrow(data.Set2U))</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; species[which(data.Set2U$QUAG == 1)] &lt;- &#8220;QUAG&#8221;</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; species[which(data.Set2U$QUWI == 1)] &lt;- &#8220;QUWI&#8221;</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; species[which(data.Set2U$QULO == 1)] &lt;- &#8220;QULO&#8221;</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; species[which(data.Set2U$QUDO == 1)] &lt;- &#8220;QUDO&#8221;</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; species[which(data.Set2U$QUKE == 1)] &lt;- &#8220;QUKE&#8221;</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; species[which(data.Set2U$QUCH == 1)] &lt;- &#8220;QUCH&#8221;</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; data.Set2U$species &lt;- as.factor(species)</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; table(data.Set2U$species)</span><br />
<span style="font-size: 10pt"><strong><span style="font-family: courier new, courier, monospace">QUAG QUCH QUDO QUKE QULO QUWI</span><br />
<span style="font-family: courier new, courier, monospace">551   99  731  717   47  122</span></strong></span>

<span style="font-size: 10pt"><strong> </strong></span>

<br />
<br />
As can be seen, the data set is highly unbalanced. This property always poses a severe challenge for classification algorithms. The data are also highly autocorrelated spatially. In the Additional Topic on Method Comparison we found that 83% of the data records have the same species classification as their nearest neighbor.

As in other Additional Topics, we want to be able to take advantage of locational information both through spatial relationships and by including <span style="font-size: 12pt;font-family: courier new, courier, monospace">Latitude </span>and <span style="font-size: 12pt;font-family: courier new, courier, monospace">Longitude </span>in the set of predictors. I won’t repeat the discussion about this since it is covered in the other Additional Topics. We will use the same <span style="font-size: 12pt;font-family: courier new, courier, monospace">spatialPointsDataFrame</span> (Bivand et al, 2011) <span style="font-size: 12pt;font-family: courier new, courier, monospace">D</span> of rescaled predictors and species class labels that was used in the Additional Topic on method comparison. Again, if you are not familiar with the <span style="font-size: 12pt;font-family: courier new, courier, monospace">spatialPointsDataFrame</span> object, don&#8217;t worry about it: you can easily understand what is going anyway.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; library(spdep)<br />
&gt; coordinates(data.Set2U) &lt;- <br />
+   c(&#8220;Longitude&#8221;, &#8220;Latitude&#8221;)<br />
&gt; proj4string(data.Set2U) &lt;-<br />
+    CRS(&#8220;+proj=longlat +datum=WGS84&#8221;)<br />
&gt; D &lt;- cbind(coordinates(data.Set2U),<br />
+    data.Set2U@data[,c(2:19, 28:34, 43)])<br />
&gt; for (i in 1:2)) D[,i] &lt;- rescale(D[,i]) <br />
<br />
</span>

&nbsp;

In this section we will use the predictors selected as optimal in the variable selection process for support vector machine analysis. This established as optimal the predictors <span style="font-size: 12pt;font-family: courier new, courier, monospace">Precip</span>, <span style="font-size: 12pt;font-family: courier new, courier, monospace">CoastDist</span>, the distance from the Pacific Coast, <span style="font-size: 12pt;font-family: courier new, courier, monospace">TempR</span>, the average annual temparature range, <span style="font-size: 12pt;font-family: courier new, courier, monospace">Elevation</span>, and <span style="font-size: 12pt;font-family: courier new, courier, monospace">Latitude</span>. We will first try out <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet()</span> using these predictors (Fig. 26). After playing around a bit with the number of hidden and skip layers, here is a result.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; modf.nnet &lt;- nnet(species ~ Precip + <br />
+    CoastDist + TempR + <span>Elevation + Latitude, </span></span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+    data = D, size = 10, skip = 4,</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+    maxit = 10000)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong># weights:  156<br />
initial  value 3635.078808<br />
iter2190 value 925.764437<br />
     *     *     *     *<br />
final  value 925.760573<br />
converged<br />
</strong>&gt; plotnet(modf.nnet, cex = 0.6) </span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><em></em>&gt; Predict.nnet &lt;- predict(modf.nnet, D)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; pred.nnet &lt;- apply(Predict.nnet, 1, <br />
+      which.max)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; 1 &#8211; (sum(diag(conf.mat)) / nrow(D))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 0.1433613<br />
</strong>&gt; print(conf.mat &lt;- table(pred.nnet, D$species))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>pred.nnet QUAG QUCH QUDO QUKE QULO QUWI<br />
        1  514    9   29    7   16   24<br />
        2    1   27    2    6    2    1<br />
        3   27    7  661   19   11   22<br />
        4    1   54   27  673    4   20<br />
        5    1    0    4    0   12    0<br />
        6    7    2    8   12    2   55<br />
</strong>&gt; print(diag(conf.mat) / <br />
+   table(data.Set2U$species), digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>QUAG QUCH QUDO QUKE QULO QUWI<br />
0.93 0.27 0.90 0.94 0.26 0.45</strong></span>

<strong> </strong>

<br />
<br />
The overall error rate is about 0.14, and, as usual, the rare species are not predicted particularly well.

 <img data-attachment-id="2132" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-252/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-252.jpg?fit=317%2C245&amp;ssl=1" data-orig-size="317,245" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 252" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-252.jpg?fit=317%2C245&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-252.jpg?fit=317%2C245&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/10/Fig-252.jpg?resize=317%2C245" alt="" class="size-medium wp-image-2132 aligncenter" width="317" height="245" data-recalc-dims="1" />

Figure 26. Schematic of the multicategory ANN.<br />
<br />


&nbsp;

In the Additional Topic on method comparison it was noted that, owing to the high level of spatial autocorrelation of these data, one can obtain an error rate of about 0.18 by simply predicting that a particular data record will be the same species as that of its nearest neighbor. We will try using that to our advantage by adding the species of the nearest and second nearest geographic neighbors to the set of predictors.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; start_time &lt;- Sys.time()</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; modf.nnet &lt;- nnet(species ~ Precip + <br />
+    CoastDist + TempR + </span><span style="font-size: 12pt;font-family: courier new, courier, monospace">Elevation + Latitude +<br />
+    nearest1 + nearest2,</span><span style="font-size: 12pt;font-family: courier new, courier, monospace"> data = D, size = 12,<br />
+    skip = 2, maxit = 10000)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; stop_time &lt;- Sys.time()</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; stop_time &#8211; start_time</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>Time difference of 11.64763 secs<br />
</strong>&gt; plotnet(modf.nnet, cex = 0.6)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; Predict.nnet &lt;- predict(modf.nnet, D)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; pred.nnet &lt;- apply(Predict.nnet, 1,<br />
+    which.max)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(conf.mat &lt;- table(pred.nnet,<br />
+    D$species))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>pred.nnet QUAG QUCH QUDO QUKE QULO QUWI<br />
        1  527    3   13    4   11   12<br />
        2    1   48    0    3    1    0<br />
        3   12    2  695    7    7   18<br />
        4    1   45   17  698    1   16<br />
        5    6    0    2    0   27    0<br />
        6    4    1    4    5    0   76<br />
</strong>&gt; 1 &#8211; (sum(diag(conf.mat)) / nrow(D))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 0.08645787<br />
</strong>&gt; print(diag(conf.mat) / <br />
+     table(data.Set2U$species), digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>QUAG QUCH QUDO QUKE QULO QUWI<br />
0.96 0.48 0.95 0.97 0.57 0.62</strong></span>

<strong> </strong>

<br />
<br />
The error rate is a little under 9%, which is really quite good. The improvement is particularly good for the rare species. Of course, this only works if one knows the species of the nearest neighbors! Nevertheless, it does show that incorporation of spatial information, if possible, can work to one’s advantage.

The <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet </span>ANN was not as successful in my limited testing. The best results were obtained when nearest neighbors were not used. The code is not shown, but here are the results.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; stop_time &#8211; start_time</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>Time difference of 2.726899 mins<br />
</strong>&gt; print(conf.mat &lt;- table(pred.neuralnet,<br />
+    D$species))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>pred.neuralnet QUAG QUCH QUDO QUKE QULO QUWI<br />
             1  499   22   31   16   21   35<br />
             2    0    8    2    2    0    0<br />
             3   51    5  654   27   22   47<br />
             4    1   62   34  672    4   23<br />
             6    0    2   10    0    0   17<br />
</strong>&gt; 1 &#8211; (sum(diag(conf.mat[1:4,1:4])) +</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+         conf.mat[5,6]) / nrow(D)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 0.1839435</strong></span>

<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong> </strong></span>

<br />
<br />
No records were predicted to be <em>QULO</em>. The performance of the SNNS MLP was about the same. The RBF ANN performed even worse.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(conf.mat &lt;- table(pred.SNNS,<br />
+   D$species))<strong><br />
pred.SNNS QUAG QUCH QUDO QUKE QULO QUWI<br />
        1  471   24   42   18   13   33<br />
        3   80   15  665   63   31   68<br />
        4    0   60   24  636    3   21<br />
</strong>&gt; 1 &#8211; (conf.mat[1:1] +conf.mat[2,3] +<br />
+      conf.mat[3,4]) / nrow(D)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 0.2183502</strong></span>

<strong> </strong>

<br />
<br />
Once again, despite being the oldest and arguably the simplest package, <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet </span>has performed the best in our analysis. There may, of course, be problems for which it does not do as well as the alternatives. Nevertheless, we will limit our remaining work to this package alone. The last topic we need to cover is variable selection for ANNs.

<h2>6. Variable Selection for ANNs</h2>

There have been numerous papers published on variable selection for ANNs (try googling the topic!). Olden and Jackson (2002), Gevery et al. (2003), and Olden et al. (2004) provide, in my opinion, particularly useful ones for the practitioner. Gevery et al. (2003) discuss some stepwise selection methods in which variables enter and leave the model based on the mean squared error (see SDA2 Sec. 8.2.1 for a discussion of stepwise methods in linear regression). These methods actually have much to recommend them, but they are not much different from stepwise selection methods we have discussed in SDA2 and in other Additional Topics and we won’t consider them further here.

The unique distinguishing feature of ANNs in terms of variable selection is their connection weights, and several methods for estimating variable importance based on these weights have been developed. The most commonly used methods for ANNs involve a single output variable, implying only binary classification or regression. We will therefore discuss them by returning to the <em>QUDO</em> classification problem of Sections 2-4. We will carry over the data frame <span style="font-size: 12pt;font-family: courier new, courier, monospace">D</span> from Section 5. Our model will include three predictors. Two of them are <em>MAT </em>and <em>Precip</em>, as used in Sections 2-4. The third, called <em>Dummy</em>, is just that: a dummy variable consisting of a uniformly distributed set of random numbers having no predictive power at all. A good method should identify <em>Dummy</em> as a prime candidate for elimination.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; D$Dummy &lt;- runif(nrow(D))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; size &lt;- 2</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; set.seed(1)</span>

&nbsp;

<br />
<br />
For simplicity our demonstration ANN will have only two hidden cells (Fig. 27). We will first select the random number seed with the best performance, using the same procedure as that carried out in Section 2. Here is the result.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; seeds [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20&gt; sort(error.rate)<br />
<strong> [1] 0.2077636 0.2077636 0.2082047 0.2254080 0.2271725 0.2271725 0.2271725<br />
[8] 0.2271725 0.2271725 0.2271725 0.2271725 0.2271725 0.2276136 0.2280547<br />
[15] 0.2284958 0.2289369 0.3224526 0.3224526 0.3224526 0.3224526<br />
</strong>&gt; print(best.seed &lt;- which.min(error.rate))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 10<br />
</strong>&gt; mod.nnet &lt;- nnet(X.t, Y.t, size = size,<br />
+     maxit = 10000)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; plotnet(mod.nnet, cex_val = 0.75) <em><br />
<br />
</em></span>

&nbsp;

Fig. 27 shows the <span style="font-size: 12pt;font-family: courier new, courier, monospace">plotnet()</span> plot of this ANN. Here are the numerical values of the biases and weights<br />
<br />
<br />


<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; mod.nnet$wts<strong> [1] -80.82405907   0.87215226  85.41369339 108.51778012  -4.8336825<br />
[6]   0.04523967   2.83945431  -0.84126067  -2.81207163  -2.34322065<br />
[11]  71.37131603</strong></span>

<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong> </strong></span>

<br />
<br />
The order of biases and weights is {B1H1, I1H1, I2H1, I3H1, B1H2, I1H2 I2H2, I3H2, B2O1, H1O1, H2O1}.

<p style="text-align: center"><strong> <img data-attachment-id="2131" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-28/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-28.jpg?fit=348%2C216&amp;ssl=1" data-orig-size="348,216" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 28" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-28.jpg?fit=348%2C216&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-28.jpg?fit=348%2C216&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-28.jpg?resize=348%2C216" alt="" class="alignnone size-full wp-image-2131" width="348" height="216" data-recalc-dims="1" /></strong></p>

Figure 27. <span style="font-size: 12pt;font-family: courier new, courier, monospace">plotnet()</span> diagram of the simple ANN.<br />
<br />


&nbsp;

Since the biases act on all predictors equally, they are not considered in the algorithm.

One of the earliest ANN methods for determining the relative importance of each predictor was developed by Garson (1991) and Goh (1995). We will present this method following the discussion of Olden and Jackson (2002). The following replicates for our current case the set of tables displayed in Box 1 of that paper. First, we compute the matrix of weights.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(n.wts &lt;- length(mod.nnet$wts))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 11<br />
</strong>&gt; <em># Remove H1O1, H2O1<br />
</em>&gt; print(M.seq &lt;- seq(1,(n.wts-size)))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 1 2 3 4 5 6 7 8 9<br />
</strong>&gt; n.X &lt;- 3</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(M.out &lt;-  seq(1,(n.wts-size),<br />
+    by = n.X + 1))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1] 1 5 9<br />
</strong>&gt; print(M1 &lt;- matrix(mod.nnet$wts[M.seq[-M.out]],</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+   n.X, size), digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>       [,1]   [,2]<br />
[1,]   0.87  0.045<br />
[2,]  85.41  2.839<br />
[3,] 108.52 -0.841</strong></span>

<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong> </strong></span>

<br />
<br />
Next we compute the output weights.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(O &lt;- matrix(mod.nnet$wts<br />
+   [seq(n.wts-size+1,n.wts)],</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+   1, size), digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>     [,1] [,2]<br />
[1,] -2.3   71</strong></span>

<strong> </strong>

<br />
<br />
You may want to compare the numerical values with the sign and shade of the links in Fig. 27. Table 4 shows the weights and output in in tabular form, analogous to the first table in Box 1 of Olden and Jackson (2002) (from this you can see the reason for the convention involving the arrangement of subscripts described in Section 2).

&nbsp;

<br />
Table 4. Input – hidden – output link weights.

<table>
<tbody>
<tr>
<td width="208">
&nbsp;
</td>
<td width="208">
Hidden H1
</td>
<td width="208">
Hidden H2
</td>
</tr>
<tr>
<td width="208">
Input I1
</td>
<td width="208">
<em>w<sup>(1)</sup><sub>11</sub> = </em>0.87
</td>
<td width="208">
<em>w<sup>(1)</sup><sub>12</sub> = </em>0.045
</td>
</tr>
<tr>
<td width="208">
Input I2
</td>
<td width="208">
<em>w<sup>(1)</sup><sub>21</sub> = </em>84.41
</td>
<td width="208">
<em>w<sup>(1)</sup><sub>22</sub> = </em>2.839
</td>
</tr>
<tr>
<td width="208">
Input I3
</td>
<td width="208">
<em>w<sup>(1)</sup><sub>31</sub> = </em>108.52
</td>
<td width="208">
<em>w<sup>(1)</sup><sub>32</sub> = &#8211;</em>0.841
</td>
</tr>
<tr>
<td width="208">
Output
</td>
<td width="208">
<em>w<sup>(2)</sup><sub>1</sub> = </em>-2.3
</td>
<td width="208">
<em>w<sup>(2)</sup><sub>2</sub> = </em>71
</td>
</tr>
</tbody>
</table>

&nbsp;

&nbsp;

If your output is different from mine then of course your table will have different values, but you should be able to check them by comparing your output with mine. Next we compute the matrix <span style="font-size: 12pt;font-family: courier new, courier, monospace">C.m</span>, which contains the contribution of each input cell to the output cell (C is a reserved word in R and can’t be used).<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; C.m &lt;- matrix(0, n.X, size)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; for (i in 1:n.X) C.m[i,] &lt;- M1[i,] * O[1,]</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(C.m, digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>     [,1]  [,2]<br />
[1,]   -2   3.2<br />
[2,] -200 202.7<br />
[3,] -254 -60.0</strong></span>

&nbsp;

<br />
<br />
These row sums represent the total weight that each input applies to the computation of the output.

Next, we compute the matrix <span style="font-size: 12pt;font-family: courier new, courier, monospace">R</span>, which is defined the ratio of the absolute value of each element of <span style="font-size: 12pt;font-family: courier new, courier, monospace">C.m</span> to the column sum. This is considered to be the relative contribution of each input cell to the activation of each output cell. We won’t be generalizing this code, and it is easier to write it using the fixed values of <span style="font-size: 12pt;font-family: courier new, courier, monospace">n.X</span> and <span style="font-size: 12pt;font-family: courier new, courier, monospace">size</span>.<br />
<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; R &lt;- matrix(0, 3, 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; for (i in 1:3) R[i,] &lt;-</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+    abs(C.m[i,]) / (abs(C.m[1,]) +</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+    abs(C.m[2,]) + abs(C.m[3,]))</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(R, digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>       [,1]  [,2]<br />
[1,] 0.0045 0.012<br />
[2,] 0.4385 0.762<br />
[3,] 0.5571 0.226</strong></span>

<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong> </strong></span>

<br />
<br />
Next, we compute the matrix <span style="font-size: 12pt;font-family: courier new, courier, monospace">S</span>, which is the sum of the relative contributions of each input cell to the activation of each output cell.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; S &lt;- matrix(0, 3, 1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; for (i in 1:3) S[i] &lt;- R[i,1] + R[i,2]</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(S, digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>      [,1]<br />
[1,] 0.017<br />
[2,] 1.201<br />
[3,] 0.783</strong></span>

<strong> </strong>

<br />
<br />
Finally, we compute the matrix <span style="font-size: 12pt;font-family: courier new, courier, monospace">RI</span>, which is the relative importance of each input cell to the activation of each output cell.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; RI &lt;- matrix(0, 3, 1)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; for (i in 1:3) RI[i,1] &lt;- S[i,1] / sum(S)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(RI, 2)</span><br />
<strong><span style="font-size: 12pt;font-family: courier new, courier, monospace">       [,1]</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[1,] 0.0083</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[2,] 0.6003</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">[3,] 0.3914</span><br />
<br />
</strong>

The package <span style="font-size: 12pt;font-family: courier new, courier, monospace">NeuralNetTools</span> contains the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">garson()</span>, which carries out these computations.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace"><br />
&gt; print(garson(mod.nnet, bar_plot = FALSE),<br />
+   digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>       rel_imp<br />
Dummy   0.0083<br />
MAT     0.6003<br />
Precip  0.3914</strong></span>

<strong> </strong>

<br />
The function <span style="font-size: 12pt;font-family: courier new, courier, monospace">garson()</span> can also be used to create a barplot of these values (Fig. 29a).<br />
<br />


&nbsp;

<img data-attachment-id="2138" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-29a/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29a.jpg?fit=239%2C239&amp;ssl=1" data-orig-size="239,239" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 29a" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29a.jpg?fit=239%2C239&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29a.jpg?fit=239%2C239&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29a.jpg?resize=173%2C173" alt="" class="alignnone  wp-image-2138" width="173" height="173" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29a.jpg?w=239&amp;ssl=1 239w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29a.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 173px) 100vw, 173px" data-recalc-dims="1" />   <img data-attachment-id="2139" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-29b/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29b.jpg?fit=236%2C236&amp;ssl=1" data-orig-size="236,236" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 29b" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29b.jpg?fit=236%2C236&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29b.jpg?fit=236%2C236&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29b.jpg?resize=170%2C170" alt="" class="alignnone  wp-image-2139" width="170" height="170" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29b.jpg?w=236&amp;ssl=1 236w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-29b.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 170px) 100vw, 170px" data-recalc-dims="1" /><br />
                     (a)                                             (b)

<br />
Fig. 29. a) barplot of Garson importance values; b) barplot of Olden-Jackson importance values.<br />
<br />
<br />


&nbsp;

Olden and Jackson (2002) criticized Garson’s algorithm on the grounds that by computing absolute values of link weights it ignores the possibility that a large positive input-hidden link weight followed by a large negative hidden-output link weight might cancel each other out. In a comparison of various methods for evaluating variable importance, Olden et al. (2004) found that Garson’s algorithm fared the worst among alternatives tested. Olden and Jackson (2002) suggest a permutation test (SDA2 Sec. 3.4) as an alternative to Garson’s algorithm. In reality, this method provides useful information even without employing a permutation test, and we will present it in that way. The test involves the following steps (we will use the same numbering as Olden and Jackson).<br />
<br />


<u>Step 1</u>. Test several random seeds and select the one giving the smallest error. This step has already been carried out.

<u><br />
Step 2a</u>. Compute the matrix C.m above for the selected weight set. Again, this step has already been carried out.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(C.m, digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[,1]  [,2]<br />
[1,]   -2   3.2<br />
[2,] -200 202.7<br />
[3,] -254 -60.0</strong></span>

<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong> </strong></span>

<u><br />
<br />
Step 2b</u>. Compute the row sums of <span style="font-size: 12pt;font-family: courier new, courier, monospace">C.m</span>.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(C.sum &lt;- apply(C.m, 1, sum),<br />
+      digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>[1]    1.2    2.5 -314.3</strong></span>

&nbsp;

<br />
<br />
The package <span style="font-size: 12pt;font-family: courier new, courier, monospace">NeuralNetTools</span> has a function <span style="font-size: 12pt;font-family: courier new, courier, monospace">olden()</span> to carry out these computations and to plot a barplot (Fig. 29b).<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; print(olden(mod.nnet, bar_plot = FALSE), digits = 2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace"><strong>       importance<br />
Dummy         1.2<br />
MAT           2.5<br />
Precip     -314.3</strong></span>

&nbsp;

<br />
<br />
In this particular example, one might say that the Garson method acquits itself better than the Olden-Jackson method. This shows that one must keep an open mind when playing with these toys.

Lek (1996) developed a method that is also discussed by Gevery et al. (2003). We will follow the discussion in this latter publication as well as the <em>Details </em>section of <span style="font-size: 12pt;font-family: courier new, courier, monospace">?lekprofile</span>. It is easiest to present the method via an example. The package <span style="font-size: 12pt;font-family: courier new, courier, monospace">NeuralNetTools</span> contains the function <span style="font-size: 12pt;font-family: courier new, courier, monospace">lekprofile()</span> that does the computations and sets up the graphical output. The default output of the function is a <span style="font-size: 12pt;font-family: courier new, courier, monospace">ggplot</span> (Wickham, 2016) object.<br />
<br />


<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; library(ggplot2)</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">&gt; lekprofile(mod, steps = 5,</span><br />
<span style="font-size: 12pt;font-family: courier new, courier, monospace">+    group_vals = seq(0, 1, by = 0.5)) </span>

&nbsp;

<br />
<br />
The figure is shown below after the explanation.

The procedure divides the range of each predictor into (<span style="font-size: 12pt;font-family: courier new, courier, monospace">steps </span>– 1) equally spaced segments. In our case, <span style="font-size: 12pt;font-family: courier new, courier, monospace">steps </span>= 5, so there are 4 segments and 5 steps (the default <span style="font-size: 12pt;font-family: courier new, courier, monospace">steps </span>value is 100). It is also possible to obtain the numerical values to compute the values represented in the plot. Here is a portion of it.<br />
<br />


<span style="font-size: 10pt;font-family: courier new, courier, monospace">&gt; lekprofile(mod.nnet, steps = 5,</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace">+    group_vals = seq(0, 1, by = 0.5),<br />
+    val_out = TRUE)</span><br />
<span style="font-size: 10pt;font-family: courier new, courier, monospace"><strong>[[1]]</strong></span>

<span style="font-size: 10pt;font-family: courier new, courier, monospace"><strong>Explanatory     resp_name    Response Groups exp_name<br />
1  0.0006052661      QUDO 0.095468020      1    Dummy<br />
2  0.2504365980      QUDO 0.096018057      1    Dummy<br />
3  0.5002679299      QUDO 0.096577123      1    Dummy<br />
4  0.7500992618      QUDO 0.097145396      1    Dummy<br />
5  0.9999305937      QUDO 0.097723053      1    Dummy<br />
6  0.0006052661      QUDO 0.189228401      2    Dummy<br />
7  0.2504365980      QUDO 0.195418722      2    Dummy<br />
8  0.5002679299      QUDO 0.201826448      2    Dummy<br />
9  0.7500992618      QUDO 0.208457333      2    Dummy<br />
10 0.9999305937      QUDO 0.215317020      2    Dummy<br />
11 0.0006052661      QUDO 0.231719048      3    Dummy<br />
         *         *            *           *<br />
30 1.0000000000      QUDO 0.263814961      3      MAT<br />
31 0.0000000000      QUDO 0.095468020      1   Precip<br />
32 0.2500000000      QUDO 0.086684129      1   Precip<br />
33 0.5000000000      QUDO 0.080092615      1   Precip<br />
34 0.7500000000      QUDO 0.017896195      1   Precip<br />
35 1.0000000000      QUDO 0.007309402      1   Precip<br />
36 0.0000000000      QUDO 0.867860787      2   Precip<br />
37 0.2500000000      QUDO 0.213604259      2   Precip<br />
38 0.5000000000      QUDO 0.119027224      2   Precip<br />
39 0.7500000000      QUDO 0.070552909      2   Precip<br />
40 1.0000000000      QUDO 0.045104426      2   Precip<br />
41 0.0000000000      QUDO 0.977067535      3   Precip<br />
42 0.2500000000      QUDO 0.902834305      3   Precip<br />
43 0.5000000000      QUDO 0.720770997      3   Precip<br />
44 0.7500000000      QUDO 0.468084251      3   Precip<br />
45 1.0000000000      QUDO 0.263814961      3   Precip</strong></span>

<span style="font-size: 10pt;font-family: courier new, courier, monospace"><strong> <br />
[[2]]<br />
            Dummy       MAT    Precip<br />
0%   0.0006052661 0.0000000 0.0000000<br />
50%  0.4781180343 0.7601916 0.2747424<br />
100% 0.9999305937 1.0000000 1.0000000</strong></span>

<span style="font-size: 10pt"><strong> </strong></span>

<br />
<br />
The numerical output is a list with two elements. The first element is a data frame and the second is a matrix. The explanation is clearest if we start with the second element. The components of the matrix are percentile values of each of the predictors. For example, the fiftieth percentile value of <span style="font-size: 12pt;font-family: courier new, courier, monospace">MAT </span>is 0.7601916. Note that the 0 and 100 percentile values of <span style="font-size: 12pt;font-family: courier new, courier, monospace">MAT </span>and <span style="font-size: 12pt;font-family: courier new, courier, monospace">Precip </span>are 0 and 1 because these variables have been rescaled. The number of percentiles is specified via the argument <span style="font-size: 12pt;font-family: courier new, courier, monospace">group_vals</span>, which has the default value <span style="font-size: 12pt;font-family: courier new, courier, monospace">seq(0, 1, by = 0.2)</span>. In the example we use a coarser sequence to make the interpretation easier.

<br />
Moving to the first element of the list, the data frame, the first data field is the column labeled <span style="font-size: 12pt;font-family: courier new, courier, monospace">explanatory</span>. Its first five values represent the subdivision into five (as specified by the value of <span style="font-size: 12pt;font-family: courier new, courier, monospace">steps</span>) equal segments of the range of values of <span style="font-size: 12pt;font-family: courier new, courier, monospace">Dummy </span>from the 0% to the 100% percentile. For each explanatory variable, the data field <span style="font-size: 12pt;font-family: courier new, courier, monospace">exp_name</span>, there are three <span style="font-size: 12pt;font-family: courier new, courier, monospace">Groups</span>; the 0, 50 and 100 percentile groups, as specified by the <span style="font-size: 12pt;font-family: courier new, courier, monospace">group_vals</span> argument. The values of the input to the <span style="font-size: 12pt;font-family: courier new, courier, monospace">mod.nnet</span> ANN object are cycled as follows. Starting with <span style="font-size: 12pt;font-family: courier new, courier, monospace">MAT</span>, this variable is first set to its 0 percentile value, which is 0. This forms Group 1 for <span style="font-size: 12pt;font-family: courier new, courier, monospace">MAT</span>. Holding MAT at this value, the other two inputs are both set successively to the values specified by steps: 0, 0.25, 0.5, 0.75, and 1. The output of the ANN is calculated for each of these combinations of values and recorded in the <span style="font-size: 12pt;font-family: courier new, courier, monospace">Response</span> data field. Thus, for example, the second data record has <span style="font-size: 12pt;font-family: courier new, courier, monospace">Response</span> = 0.096018057, which is the value of <span style="font-size: 12pt;font-family: courier new, courier, monospace">QUDO</span> computed by the ANN for the input vector {<span style="font-size: 12pt;font-family: courier new, courier, monospace">Dummy, MAT, Precip</span>} = {0, 0.25, 0.25}. This value of <span style="font-size: 12pt;font-family: courier new, courier, monospace">Response </span>is encircled by the small black circle.<br />
<br />


<img data-attachment-id="2141" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-30a/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30a.jpg?fit=247%2C247&amp;ssl=1" data-orig-size="247,247" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 30a" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30a.jpg?fit=247%2C247&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30a.jpg?fit=247%2C247&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30a.jpg?resize=185%2C185" alt="" class="alignnone  wp-image-2141" width="185" height="185" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30a.jpg?w=247&amp;ssl=1 247w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30a.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 185px) 100vw, 185px" data-recalc-dims="1" />   <img data-attachment-id="2142" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/fig-30b/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30b.jpg?fit=259%2C259&amp;ssl=1" data-orig-size="259,259" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="fig 30b" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30b.jpg?fit=259%2C259&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30b.jpg?fit=259%2C259&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30b.jpg?resize=208%2C208" alt="" class="alignnone  wp-image-2142" width="208" height="208" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30b.jpg?w=259&amp;ssl=1 259w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/10/fig-30b.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 208px) 100vw, 208px" data-recalc-dims="1" /><br />
Figure 30. a) Lek plot of the three-variable model. The response to {<span style="font-size: 12pt;font-family: courier new, courier, monospace">Dummy, MAT, Precip</span>} = {0, 0.25, 0.25} is encircled by the small black circle; b) plot of the data.<br />
<br />


&nbsp;

Now we can interpret Fig. 30a. For each of the three predictors, the colored lines represent the ANN response as that predictor is held fixed and the other two predictors are successively increased in value. For Dummy there is, unsurprisingly, very little response. Discounting the effect of Dummy, at low values of MAT (Group 1), the response QUDO is high until Precip reaches a high value. Fig 30b shows (again) the data, helping to interpret the plots in Fig. 30a. The Lek profile allows us to interpret how the predictors interact to generate the value of the class label.

In terms of variable selection, we can see that the Lek plot would indeed suggest that we eliminate <span style="font-size: 12pt;font-family: courier new, courier, monospace">Dummy</span>, since the values of QUDO are virtually unresponsive to it. Depending on how we implement it, the Lek method can be used for stepwise selection, by bringing explanatory variables in and out of the model depending on how they line up in plots such as Fig. 30, or one can simply carry out comparison of all of the explanatory variables at once. The <em>Details</em> section of <span style="font-size: 12pt;font-family: courier new, courier, monospace">?lekprofile</span> describes other uses of the results of an analysis based on the Lek method.

<h2>7. Further Reading</h2>

Bishop (1995) is probably the most widely cited introductory book on ANNs. The text by Nunes da Silva et al. (2017) is also a good resource. Venables and Ripley (2002) provide an excellent discussion at a more advanced and theoretical level. A blog by Hatwell (2016) provides a nice discussion of the variable selection methods in a regression context. The manual by Zell et al. (1998), although specifically developed for the SNNS library, gives a good practical discussion of many aspects of ANNs. Finally, <a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.07154&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false">this site</a>, developed by Daniel Smilkov and Shan Carter, is a “neural net playground” where you can try out various ANN configurations and see the results (I am grateful to Kevin Ryan for pointing this site out to me).

<h2>8. Exercises</h2>

1) Repeat the analysis of Section 2 using <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet()</span>, but apply it to a data set in which <em>MAT</em> and <em>Precip</em> are normalized rather than rescaled. Determine the difference in predicted value of the class labels.

&nbsp;

2) Explore the use of the homemade ANN <span style="font-size: 12pt;font-family: courier new, courier, monospace">ANN.1()</span> with differently sized hidden layers in terms of the stability of the prediction.

&nbsp;

3) By computing the sum of squared errors for a set of predictions of <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet()</span>, determine whether it is converging to different local minima for different random starting values and number of hidden cells.

&nbsp;

4) Using a set of predictions of <span style="font-size: 12pt;font-family: courier new, courier, monospace">nnet()</span>, compare the error rates when entropy is TRUE with the default of FALSE.

&nbsp;

5) If you have access to Venables and Ripley (2002), read about the softmax option. Repeat Exercise (4) for this option.

&nbsp;

6) Generate a “bestiary” of predictions of <span style="font-size: 12pt;font-family: courier new, courier, monospace">neuralnet()</span> analogous to that of Fig. 9 for <span style="font-size: 12pt;font-family: courier new, courier, monospace">ANN1.()</span>.<br />
<br />
7) Look up the word &#8220;beastiary&#8221; in the dictionary.

<h2>9. References</h2>

Beck, M. W. (2018). NeuralNetTools: Visualization and Analysis Tools for Neural Networks. _<em>Journal of Statistical Software</em> 85: 1-20. doi: 10.18637/jss.v085.i11 (URL: <a href="https://doi.org/10.18637/jss.v085.i11">https://doi.org/10.18637/jss.v085.i11</a>).

&nbsp;

Bishop, C.M. (1995). <em>Neural Networks for Pattern Recognition</em>. Clarendon Press, Oxford, UK.

&nbsp;

Bergmeir, C. and J. M. Benitez (2012). Neural Networks in R Using the Stuttgart Neural Network Simulator: RSNNS. <em>Journal of Statistical Software</em>, 46(7): 1-26. URL <a href="http://www.jstatsoft.org/v46/i07/">http://www.jstatsoft.org/v46/i07/</a>

&nbsp;

Bivand, R. (with contributions by M. Altman, L. Anselin, R. Assunçăo, O. Berke, A. Bernat, G. Blanchet, E. Blankmeyer, M. Carvalho, B. Christensen, Y. Chun, C. Dormann, S. Dray, R. Halbersma, E. Krainski, N. Lewin-Koh, H. Li, J. Ma, G. Millo, W. Mueller, H. Ono, P. Peres-Neto, G. Piras, M. Reder, M. Tiefelsdorf and and D. Yu) (2011). spdep: Spatial dependence: weighting schemes, statistics and models. <a href="http://CRAN.R-project.org/package=spdep">http://CRAN.R-project.org/package=spdep</a>.

&nbsp;

Du, K.-L. and M.N.S. Swamy (2014). <em>Neural Networks and Statistical Learning</em>. Springer, London

&nbsp;

Fritsch, S., F. Guenther and M. N. Wright (2019). neuralnet: Training of Neural Networks. R package version 1.44.2. <a href="https://CRAN.R-project.org/package=neuralnet">https://CRAN.R-project.org/package=neuralnet</a>

&nbsp;

Gevery, M. I. Dimopoulos and S. Lek (2003). Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological Modelling 160: 249-264.

Guenther, F. and S. Fritsch (2010). neuralnet: Training of Neural Networks. <em>The R Journal</em> 2:30-38.

&nbsp;

Hatwell, J. (2016) Artificial Neural Networks in R. <a href="https://rpubs.com/julianhatwell/annr">https://rpubs.com/julianhatwell/annr</a>

&nbsp;

Meyer, D, E. Dimitriadou, K. Hornik, A. Weingessel and F. Leisch (2019). e1071: Misc Functions  of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. R package version 1.7-3. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>

&nbsp;

Nunes DaSilva, I.N, D. Hernane Spatti, R. Andrade Flauzino, L.H. Bartocci Liboni and S.F. dos Reis Alves (2017). <em>Artificial Neural Networks: A Practical Course. </em>Springer International, Switzerland.

&nbsp;

Riedmiller M. and H. Braun (1993) A direct adaptive method for faster backpropagation learning: The RPROP algorithm. <em>Proceedings of the IEEE International Conference on Neural Networks</em>: 586-591. San Francisco, CA.

&nbsp;

Ripley, B.D. (1996). <em>Pattern Recognition and Neural Networks</em>. Cambridge University Press, Cambridge, UK

&nbsp;

Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. <em>Psychological Review, 65</em>(6), 386–408. <a href="https://psycnet.apa.org/doi/10.1037/h0042519">https://doi.org/10.1037/h0042519</a>

&nbsp;

Venables, W. N. and B.D. Ripley (2002) <em>Modern Applied Statistics with S</em>. Fourth Edition. Springer, New York.

&nbsp;

Wickham, H. (2016). <em>ggplot2: Elegant Graphics for Data Analysis</em>. Springer-Verlag New York.

&nbsp;

Wieslander, A. E. (1935). A vegetation type map of California. <em>Madroño</em> 3: 140-144.

&nbsp;

Zell, A. and 27 others (1998), SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2. IPVR, University of Stuttgart and WSI, University of Tubingen. URL <a href="http://www.ra.cs.uni-tuebingen.de/SNNS/">http://www.ra.cs.uni-tuebingen.de/SNNS/</a>

&nbsp;

&nbsp;

&nbsp;

&nbsp;<hr style="border-top:black solid 1px" /><a href="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/">Spatial Data Analysis Using Artificial Neural Networks, Part 2</a> was first posted on October 14, 2020 at 2:55 pm.<br />]]></content:encoded>
					
					<wfw:commentRss>https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-2/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Spatial Data Analysis Using Artificial Neural Networks Part 1</title>
		<link>https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/</link>
					<comments>https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/#respond</comments>
		
		<dc:creator><![CDATA[Richard Plant]]></dc:creator>
		<pubDate>Fri, 09 Oct 2020 08:52:25 +0000</pubDate>
				<category><![CDATA[R]]></category>
		<guid isPermaLink="false">http://r-posts.com/?p=2025</guid>

					<description><![CDATA[Spatial Data Analysis Using Artificial Neural Networks, Part 1 Richard E. Plant Additional topic to accompany Spatial Data Analysis in Ecology and Agriculture using R, Second Edition http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm Chapter and section references are contained in that text, which is referred to as SDA2. Data sets and R code are available in the Additional Topics through &#8230; <a href="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/" class="more-link">Continue reading <span class="screen-reader-text">Spatial Data Analysis Using Artificial Neural Networks Part 1</span></a><hr style="border-top:black solid 1px" /><a href="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/">Spatial Data Analysis Using Artificial Neural Networks Part 1</a> was first posted on October 9, 2020 at 8:52 am.<br />]]></description>
										<content:encoded><![CDATA[<h1>Spatial Data Analysis Using Artificial Neural Networks, Part 1</h1>

Richard E. Plant</p>

Additional topic to accompany <em>Spatial Data Analysis in Ecology and Agriculture using R, Second Edition<br />
</em><br />
<a href="http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm">http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm</a>

<br />
Chapter and section references are contained in that text, which is referred to as SDA2. Data sets and R code are available in the Additional Topics through the link above.

<h2>Forward</h2>

This post originally appeared as an Additional Topic to my book as described above. Because it may be of interest to a wider community than just agricultural scientists and ecologists, I am posting it here as well. However, it is too long to be a single post. I have therefore divided it into two parts, to be posted successively. I have also removed some of the mathematical derivations. This is the first of these two posts in which we discuss ANNs with a single hidden layer and construct one to see how it functions. The second post will discuss multilayer and radial basis function ANNs,  ANNs for multiclass problems, and the determination of predictor variable importance. All references are listed at the end of the second post (if you are in a hurry, you can go to the link above and get the full Additional Topic). Finally, some of the graphics and formulas became somewhat blurred when I transfered them from the original document to this post. I apologize for that, and again, if you want to see the clearer version you should go to the website listed above and download the pdf file.

<h2>1. Introduction</h2>

Artificial neural networks (ANNs) have become one of the most widely used analytical tools for both supervised and unsupervised classification. Nunes da Silva et al. (2017) give a detailed history of ANNs and the interested reader is referred to that source. One of the first functioning artificial neural networks was constructed by Frank Rosenblatt (1958), who called his creation a <em>perceptron</em>. Rosenblatt’s perceptron was an actual physical device, but it was also simulated on a computer, and after a hiatus of a decade or so work began in earnest on simulated ANNs. The term “perceptron” was retained to characterize a certain type of artificial neural network and this type has become one of the most commonly used today. Different authors use the term “perceptron” in different ways, but most commonly it refers to a type of ANN called a <em>multilayer perceptron</em>, or MLP. This is the first type of ANN that we will explore, in Section 2.<br />
             <img data-attachment-id="2027" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-1a/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1a.jpg?fit=241%2C180&amp;ssl=1" data-orig-size="241,180" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 1a" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1a.jpg?fit=241%2C180&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1a.jpg?fit=241%2C180&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1a.jpg?resize=170%2C127" alt="" class="alignnone  wp-image-2027" width="170" height="127" data-recalc-dims="1" />       <img data-attachment-id="2028" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-1b/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1b.jpg?fit=271%2C183&amp;ssl=1" data-orig-size="271,183" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 1b" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1b.jpg?fit=271%2C183&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1b.jpg?fit=271%2C183&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-1b.jpg?resize=200%2C135" alt="" class="alignnone  wp-image-2028" width="200" height="135" data-recalc-dims="1" />

                            (a)                                                                   (b)<br />
Figure 1. Two plots of the same simple ANN. (a) Display of numerical values of connections, (b) schematic representation of connection values.<br />
<br />


Fig. 1 shows two representations of a very simple ANN constructed using the R package <span style="font-family: courier new, courier, monospace; font-size: 12pt;">neuralnet </span>(Fritsch et al. 2019). The representation in Fig. 1a shows greater detail, displaying numerical values at each link, or connection, indicating the weight and sign. The representation in Fig. 1b of the same ANN but using the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">plotnet()</span> function of the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">NeuralNetTools</span> package (Beck, 2018) is more schematic, with the numerical value of the link indicated by its thickness and the sign by its shade, with black for positive and gray for negative. The circles in this context represent artificial nerve cells. If you have never seen a sketch of a typical nerve cell, you can see one <a href="https://www.datacamp.com/community/tutorials/neural-network-models-r">here</a>. In vast oversimplification, a nerve cell, when activated by some stimulus, transmits an <em>action potential</em> to other nerve cells via its <em>axons</em>. At the end of the axon are <em>synapses</em>, which release <em>synaptic vesicles</em> that are taken up by the <em>dendrites</em> of the next nerve cell in the chain. The signal transmitted in this way can be either <em>excitatory</em>, tending to make the receptor nerve cell develop an action potential, or <em>inhibitory</em>, tending to impede the production of an action potential. Multiple input cells combine their transmitted signal, and if the strength of the combined signal surpasses a <em>threshold</em> then the receptor cell initiates an action potential of its own and transmits the signal down its axon to other cells in the network.

In this context the circles in Fig. 1 represent “nerve cells” and the links correspond to axons linking one “nerve cell” to another. The “artificial neural network” of Fig. 1 is a <em>multilayer</em> perceptron because it has at least one layer between the input and the output. The circles with an <em>I</em> represent the inputs, the circles with an <em>H</em> are the “hidden” cells, so called because they are “hidden” from the observer, and circle with an <em>O</em> is the output. The numerical values beside the arrows in Fig. 1a, which are called the <em>weights</em>, represent the strength of the connection, with positive values corresponding to excitatory connections and negative values to inhibitory connections. The response of each cell is typically characterized by a nonlinear function such as a logistic function (Fig. 2). This represents schematically the response of a real nerve cell, which is nonlinear. The circles at the top in Fig. 1, denoted with a 1 in Fig. 1a and with a <em>B</em> in Fig. 1b, represent the <em>bias</em>, which is supposed to correspond to the activation threshold in a nerve cell. Obviously, a system like that represented in Fig. 1 does not rise to the level of being a caricature of the neural network of even the simplest organism. As Venables and Ripley (2002) point out, it is better to drop the biological metaphor altogether and simply think of this as a very flexible nonlinear model of the data. Nevertheless, we will conform to universal practice and use the term “artificial neural network,” abbreviated ANN, to characterize these constructs.

<p style="text-align: center;"><img data-attachment-id="2029" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-2/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-2.jpg?fit=259%2C259&amp;ssl=1" data-orig-size="259,259" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 2" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-2.jpg?fit=259%2C259&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-2.jpg?fit=259%2C259&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-2.jpg?resize=259%2C259" alt="" class="alignnone size-full wp-image-2029" width="259" height="259" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-2.jpg?w=259&amp;ssl=1 259w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-2.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 259px) 100vw, 259px" data-recalc-dims="1" /></p>

Figure 2. A logistic function.<br />
<br />


In Section 2 we introduce the topic by manually constructing a multilayer perceptron (MLP) and comparing it to an MLP constructed using the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet</span> package (Venables and Ripley, 2002), which comes with the base R software. In Section 3 (in Part 2) we will use the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">neuralnet</span> package (Fritsch et al., 2019) to develop and test an MLP with more than one hidden layer, like that in Fig. 1. Although the MLP is probably the most common ANN used for the sort of classification and regression problems discussed here, it is not the only one. In Section 4 we use the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">RSNNS</span> package (Bergmeir and Benitez, 2012) to develop a <em>radial basis function</em> ANN. This is, for these types of application, probably the second most commonly used ANN. These first three sections apply the ANN to a binary classification problem. In Section 5 (in Part 3) we examine them in a classification problem with more than two alternatives. In Section 6 we take a look at the issue of variable selection for ANNs. Section 7 contains the Exercises and Section 8 the references. We do not discuss regression problems, but the practical aspects of this sort of application are virtually identical to those of a binary classification problem.

<h2>2. The Single Hidden Layer ANN</h2>

We will start the discussion by creating a training data set like the one that we used in our <a href="https://psfaculty.plantsciences.ucdavis.edu/plant/additionaltopics_KNN.pdf">Additional Topic on the K nearest neighbor method</a> to introduce that method. The application is the classification of <a href="https://nature.berkeley.edu/garbelottowp/?p=1749">oak woodlands in California</a> using an augmented version of Data Set 2 of SDA2. In SDA2 that data set was only concerned with the presence or absence in the California landscape of blue oaks (<em>Quercus douglasii</em>), denoted in Data Set 2 by the variable <em>QUDO</em>. The original Wieslander survey on which this data set is based (Wieslander, 1935), however, contains records for other oak species as well. Here we employ an augmented version of Data Set 2, denoted Data Set 2A, that also contains records for coast live oak (<em>Quercus agrifolia, QUAG</em>), canyon oak (<em>Quercus chrysolepis, QUCH</em>), black oak (<em>Quercus kelloggii, QUKE</em>), valley oak (<em>Quercus lobata, QULO</em>), and interior live oak (<em>Quercus wislizeni, QUWI</em>). The data set contains records for the basal area of each species as well as its presence/absence, but we will only use the presence/absence data here. Of the 4,101 records in the data set, 2,267 contain only one species and we will analyze this subset, denoted Data Set 2U. If you want to see a color coded map of the locations, there is one <a href="https://psfaculty.plantsciences.ucdavis.edu/plant/method%20comparison.pdf">here</a>.<br />
<br />


In this and the following two sections we will restrict ourselves to a binary classification problem: whether or not the species at the given location is <em>QUDO. </em>In the general description we will denote the variable to be classified by <em>Y</em>. As with the other Additional Topics discussing supervised classification, we will refer to the <em>Y<sub>i</sub></em>, <em>i</em> = 1,&#8230;,<em>n</em>,  as the <em>class labels</em>. The data fields on which the classification is to be based are called the <em>predictors </em>and will in the general case be denoted <em>X<sub>i</sub></em>, . When we wish to distinguish the individual data fields we will write the components of the predictor <em>X<sub>i</sub></em> as <em>X<sub>i,j</sub></em>, <em>j</em> = 1,&#8230;,<em>p</em>. As mentioned in the Introduction, the application of ANNs to problems in which <em>Y</em> takes on continuous numerical values (i.e., to regression as opposed to classification problems) is basically the same as that described here.

Because ANNs incorporate weights like those in Fig. 1a, it is important that all predictors have approximately the same numerical range. Ordinarily this is assured by normalizing them, but for ease of graphing it will be more useful for us to rescale them to the interval 0 to 1. There is little or no difference between normalizing and rescaling as far as the results are concerned (Exercise 1). Let’s take a look at the setup of the data set.<br />
<br />
<span style="font-size: 12pt;"><span style="font-family: courier new, courier, monospace;">data.Set2A &lt;- read.csv(&#8220;set2\\set2Adata.csv&#8221;,<br />
+ header = TRUE)</span><br />
<span style="font-family: courier new, courier, monospace;">&gt; data.Set2U &lt;- data.Set2A[which(data.Set2A$QUAG + <br />
+   data.Set2A$QUWI +</span><span style="font-family: courier new, courier, monospace;"> data.Set2A$QULO +<br />
+   data.Set2A$QUDO + data.Set2A$QUKE +<br />
</span><span style="font-family: courier new, courier, monospace;"> +   data.Set2A$QUCH == 1),]</span><br />
<span style="font-family: courier new, courier, monospace;">&gt; names(data.Set2U)</span><br />
</span><span style="font-family: courier new, courier, monospace; font-size: 10pt;"><strong> [1] &#8220;ID&#8221;        &#8220;Longitude&#8221; &#8220;Latitude&#8221;  &#8220;CoastDist&#8221; &#8220;MAT&#8221;     <br />
[6] &#8220;Precip&#8221;    &#8220;JuMin&#8221;     &#8220;JuMax&#8221;     &#8220;JuMean&#8221;    &#8220;JaMin&#8221;   <br />
[11] &#8220;JaMax&#8221;     &#8220;JaMean&#8221;    &#8220;TempR&#8221;     &#8220;GS32&#8221;      &#8220;GS28&#8221;    <br />
[16] &#8220;PE&#8221;        &#8220;ET&#8221;        &#8220;Elevation&#8221; &#8220;Texture&#8221;   &#8220;AWCAvg&#8221;  <br />
[21] &#8220;Permeab&#8221;   &#8220;PM100&#8221;     &#8220;PM200&#8221;     &#8220;PM300&#8221;     &#8220;PM400&#8221;   <br />
[26] &#8220;PM500&#8221;     &#8220;PM600&#8221;     &#8220;SolRad6&#8221;   &#8220;SolRad12&#8221;  &#8220;SolRad&#8221;  <br />
[31] &#8220;QUAG&#8221;      &#8220;QUWI&#8221;      &#8220;QULO&#8221;      &#8220;QUDO&#8221;      &#8220;QUKE&#8221;    <br />
[36] &#8220;QUCH&#8221;      &#8220;QUAG_BA&#8221;   &#8220;QUWI_BA&#8221;   &#8220;QULO_BA&#8221;   &#8220;QUDO_BA&#8221; <br />
[41] &#8220;QUKE_BA&#8221;   &#8220;QUCH_BA&#8221;  </strong></span>

<span style="font-family: courier new, courier, monospace; font-size: 10pt;"><strong><br />
</strong></span><br />
Later on we are going to convert the data frame to a <span style="font-size: 12pt; font-family: courier new, courier, monospace;">SpatialPointsDataFrame</span> (Bivand et al. 2011), so we will not rescale the spatial coordinates. If you are not familiar with the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">SpatialPointsDataFrame</span> concept, don’t worry about it – it is not essential to understanding what follows.<br />
<br />
<span style="font-size: 10pt; font-family: courier new, courier, monospace;">&gt; rescale &lt;- function(x) (x &#8211; min(x)) / (max(x) &#8211; min(x))<br />
> for (j in 4:30) data.Set2U[,j] &lt;- rescale(data.Set2U[,j])<br />
> nrow(data.Set2U)<br />
<strong>[1] 2267<br />
</strong>&gt; length(which(data.Set2U$QUDO == 1)) / nrow(data.Set2U)<br />
<strong>[1] 0.3224526</strong></span>

<br />
<br />
The fraction of data records having the true value <em>QUDO </em>= 1 is about 0.32. This would be the error rate obtained by not assigning <em>QUDO </em>= 1 to any records and represents an error rate against which others can be compared.

To simplify the explanations further we will begin by using a subset of Data Set 2U called the Training data set that consists of 25 randomly selected records each of <em>QUDO</em> = 0 and <em>QUDO </em>= 1 records.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; set.seed(5)<br />
> oaks &lt;- sample(which(data.Set2U$QUDO == 1), 25)<br />
> no.oaks &lt;- sample(which(data.Set2U$QUDO == 0), 25)<br />
> Set2.Train &lt;- data.Set2U[c(oaks,no.oaks),]</span>

<br />
<br />
As usual I played with the random seed until I got a training set that gave good results.<br />
<br />


We will begin each of the first three sections by applying the ANN to a simple “practice problem” having only two predictors (i.e., <em>p</em> = 2), mean annual temperature <em>MAT</em> and mean annual precipitation <em>Precip.</em> Fig. 3 shows plots of the full and training data sets in the data space of these predictors. We begin our discussion with the oldest R ANN package, <span style="font-family: courier new, courier, monospace;">nnet</span> (Venables and Ripley, 2002), which includes the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span>. Venables and Ripley provide an excellent description of the package, and the source code is available <a href="https://github.com/cran/nnet/blob/master/R/nnet.R">here</a>. An ANN created using <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> is restricted to one hidden layer, although a “skip layer,” that is, a set of one or more links directly from the input to the output, is also allowed. A number of options are available, and we will begin with the most basic. We will start with a very simple call to <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span>, accepting the default value wherever possible. We will set the random number seed before each run of any ANN. The algorithm begins with randomly selected initial values, and this way we achieve repeatable results. Again, your results will probably differ from mine even if you use the same seed.<br />
<br />


                 <img data-attachment-id="2034" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-3a/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3a.jpg?fit=219%2C219&amp;ssl=1" data-orig-size="219,219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 3a" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3a.jpg?fit=219%2C219&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3a.jpg?fit=219%2C219&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3a.jpg?resize=183%2C183" alt="" class="alignnone  wp-image-2034" width="183" height="183" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3a.jpg?w=219&amp;ssl=1 219w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3a.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 183px) 100vw, 183px" data-recalc-dims="1" />   <img data-attachment-id="2076" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-3b-2/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3b-1.jpg?fit=219%2C219&amp;ssl=1" data-orig-size="219,219" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 3b" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3b-1.jpg?fit=219%2C219&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3b-1.jpg?fit=219%2C219&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3b-1.jpg?resize=185%2C185" alt="" class="alignnone  wp-image-2076" width="185" height="185" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3b-1.jpg?w=219&amp;ssl=1 219w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-3b-1.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 185px) 100vw, 185px" data-recalc-dims="1" /><br />
                                     (a)                                       (b)<br />
<br />


Figure 3. (a) Plot of the full data set, (b) plot of the training data set.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; library(nnet)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; set.seed(3)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; mod.nnet &lt;- nnet(QUDO ~ MAT + Precip, <br />
+   data = Set2.Train, size = 4)</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;"># weights:  17</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">initial  value 12.644382</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">iter  10 value 8.489889</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">      *      *      *</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">iter 100 value 6.158126</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">final  value 6.158126</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">stopped after 100 iterations</span></strong><br />

<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> </span></strong>

<br />
If you are not familiar with the R formula notation <span style="font-size: 12pt; font-family: courier new, courier, monospace;">QUDO ~ MAT + Precip</span> see <a href="https://faculty.chicagobooth.edu/richard.hahn/teaching/formulanotation.pdf">here</a>. As already stated, <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> allows only one hidden layer, and the argument <span style="font-size: 12pt; font-family: courier new, courier, monospace;">size = 4</span> specifies four “cells” in this layer. From now on to avoid being pedantic we will drop the quotation marks around the word cell. The output tells us that there are 17 weights to iteratively compute (we will see in a bit where this figure comes from), and <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> starts with a default maximum value of 100 iterations, after which the algorithm has failed to converge. Let’s try increasing the value of the maximum number of iterations as specified by the argument <span style="font-size: 12pt; font-family: courier new, courier, monospace;">maxit</span>.

<p style="text-align: left;"><span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; set.seed(3)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; mod.nnet &lt;- nnet(QUDO ~ MAT + Precip,<br />
+   data = Set2.Train,</span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> size = 4, maxit = 10000)</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;"># weights:  17</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">initial  value 12.644382</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">iter  10 value 8.489889</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">     *      *       *</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">iter 170 value 6.117850</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">final  value 6.117849</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">converged</span></strong><br />
 <br />
This time it converges. Sometimes it does and sometimes it doesn’t, and again your results may be different from mine. <br />
<br />
                          <img data-attachment-id="2049" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-4/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-4.jpg?fit=324%2C203&amp;ssl=1" data-orig-size="324,203" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 4" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-4.jpg?fit=324%2C203&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-4.jpg?fit=324%2C203&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-4.jpg?resize=324%2C203" alt="" class="alignnone size-full wp-image-2049" width="324" height="203" data-recalc-dims="1" /><br />
<br />
</p>

Fig. 4. Schematic diagram of an ANN with a single hidden layer having four cells.<br />
<br />


<br />
Fig. 4 shows a plot of the ANN made using the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">plotnet()</span> function. Again, the thickness of the lines indicates the strength of the corresponding link, and the color indicates the direction, with black indicating positive and gray indicating negative. Thus, for example, <em>MAT </em>has a strong positive effect on hidden cell 3 and <em>Precip</em> has a strong negative effect. The function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> returns a lot of information, some of which we will explore later. The <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet </span>package includes a <span style="font-size: 12pt; font-family: courier new, courier, monospace;">predict()</span> function similar to the ones encountered in packages covered in other Additional Topics. We will use it to generate values of a test data set. Unlike other Additional Topics, the test data set here will not be drawn from the <em>QUDO</em> data. Instead, we will cover the data space uniformly so that we can see the boundaries of the predicted classes and compare them with the values of the training data set. As is usually true, there is an R function available that can be called to carry out this operation (in this case it is <span style="font-size: 12pt; font-family: courier new, courier, monospace;">expand.grid()</span>). However, I do not see any advantage in using functions like this to perform tasks that can be accomplished more transparently with a few lines of code.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; n &lt;- 50</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; MAT.test &lt;- rep((1:n), n) / n</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; Precip.test &lt;- sort(rep((1:n), n) / n)     </span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; Set2.Test &lt;- data.frame(MAT = MAT.test,<br />
+   Precip = Precip.test)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; Predict.nnet &lt;- predict(mod.nnet, Set2.Test)</span><br />
<br />

Let’s first take a look at the structure of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">Predict.nnet</span>.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; str(Predict.nnet)</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> num [1:2500, 1] 0 0 0 0 0 0 0 0 0 0 &#8230;</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> &#8211; attr(*, &#8220;dimnames&#8221;)=List of 2</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">  ..$ : chr [1:2500] &#8220;1&#8221; &#8220;2&#8221; &#8220;3&#8221; &#8220;4&#8221; &#8230;</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">  ..$ : NULL</span></strong><br />

&nbsp;

It is a 2500 x 1 array, with rows named 1 through 2500. We can examine the range of values by constructing a histogram (Fig. 5)

<p style="text-align: left;"><br />
<img data-attachment-id="2050" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-5/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig.-5.jpg?fit=321%2C321&amp;ssl=1" data-orig-size="321,321" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig. 5" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig.-5.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig.-5.jpg?fit=321%2C321&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig.-5.jpg?resize=215%2C215" alt="" class="wp-image-2050 aligncenter" width="215" height="215" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig.-5.jpg?resize=300%2C300&amp;ssl=1 300w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig.-5.jpg?resize=150%2C150&amp;ssl=1 150w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig.-5.jpg?w=321&amp;ssl=1 321w" sizes="(max-width: 215px) 100vw, 215px" data-recalc-dims="1" /><br />
<br />
Figure 5. Histogram of values of Predict.nnet.</p>

This shows a range of values between 0 and 1. Those values above a certain threshold will be interpreted as <em>QUDO</em> = 1. With more complex models we will choose the threshold by constructing a receiver operating characteristic curve (i.e., a <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a>), but for this simple situation it is pretty clear that the value one half will suffice. Since we will be constructing many plots of a similar nature, we will develop a function to carry out this process.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; plot.ANN &lt;- function(Data, Pred, Thresh,<br />
+      header.str){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+   Data$Color &lt;- &#8220;red&#8221;</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+   Data$Color[which(Pred &lt; Thresh)] &lt;- &#8220;green&#8221;</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+   with(Data, plot(x = MAT, y = Precip, pch = 16,<br />
+     col = Color,</span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> cex = 0.5, main = header.str))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+   }  </span><br />

<br />
Next we apply the function.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; plot.ANN(Set2.Test, Predict.nnet, 0.5, &#8220;nnet<br />
+     Predictions&#8221;)<em> # Fig. 6</em></span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; with(Set2.Train, points(x = MAT, y = Precip,<br />
+    pch = 16, col = Color))</span><br />
<br />
<br />
<img data-attachment-id="2051" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-6/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-6.jpg?fit=592%2C395&amp;ssl=1" data-orig-size="592,395" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 6" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-6.jpg?fit=450%2C300&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-6.jpg?fit=450%2C300&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-6.jpg?resize=520%2C346" alt="" class=" wp-image-2051 aligncenter" width="520" height="346" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-6.jpg?resize=450%2C300&amp;ssl=1 450w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-6.jpg?w=592&amp;ssl=1 592w" sizes="(max-width: 520px) 100vw, 520px" data-recalc-dims="1" /><br />

Figure 6. A “bestiary” of predictions delivered by the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span>. Most of the time the function returns “boring” predictions like the first and fourth, but occasionally an interesting one is produced.<br />
<br />


If we run the code sequence several times, each time using a different random number seed, each run generates a different set of starting weights and returns a potentially different prediction. Fig. 6 shows some of them. Why does this happen? There are a few possibilities. The plots in Fig. 6 represent the results of the solution of an unconstrained nonlinear minimization problem (we will delve into this problem more deeply in a bit). Solving a problem such as this is analogous to trying to find the lowest point in hilly terrain in a dense fog. Moreover, this search is taking place in the seventeen-dimensional space of the biases and weights. It is possible that some of the solutions represent local minima, and some may also arise if the area around the global minimum is relatively flat so that convergence is declared before reaching the true minimum. It is, of course, also possible that the true global minimum gives rise to an oddly shaped prediction. Exercise 3 explores this issue a bit more deeply.<br />
<br />


Now that we have been introduced to some of the concepts, let’s take a deeper look at how a multilayer perceptron ANN works. Since each individual cell is a logistic response function, we can start by simply fitting a logistic curve to the data.  Logistic regression is discussed in SDA Sec. 8.4. The logistic regression model is generally written as (cf. SDA2 Equation 8.20)<br />
<br />


<table style="width: 87.8843%;">
<tbody>
<tr>
<td style="width: 76.8535%;" width="695">
                                   <img data-attachment-id="2043" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/logrg/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/logrg.jpg?fit=220%2C49&amp;ssl=1" data-orig-size="220,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="logrg" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/logrg.jpg?fit=220%2C49&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/logrg.jpg?fit=220%2C49&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/logrg.jpg?resize=220%2C49" alt="" class="alignnone size-full wp-image-2043" width="220" height="49" data-recalc-dims="1" />,
</td>
<td style="width: 10.8499%;" width="90">
(1)
</td>
</tr>
</tbody>
</table>

where <span style="font-family: georgia, palatino, serif;"><em><img data-attachment-id="2044" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/pi/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/pi.jpg?fit=22%2C28&amp;ssl=1" data-orig-size="22,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pi" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/pi.jpg?fit=22%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/pi.jpg?fit=22%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/pi.jpg?resize=22%2C28" alt="" class="alignnone size-full wp-image-2044" width="22" height="28" data-recalc-dims="1" /></em></span> is the logistic regression estimator of <img data-attachment-id="2045" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/pry/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/pry.jpg?fit=63%2C28&amp;ssl=1" data-orig-size="63,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="pry" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/pry.jpg?fit=63%2C28&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/pry.jpg?fit=63%2C28&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/pry.jpg?resize=63%2C28" alt="" class="alignnone size-full wp-image-2045" width="63" height="28" data-recalc-dims="1" />. Based on the discussion in SDA2 we will use the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">glm()</span> of the base package to determine the logistic model for our training data set.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; mod.glm &lt;- glm(QUDO ~ MAT + Precip,</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    data = Set2.Train, family = binomial)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; Predict.glm &lt;- predict(mod.glm, Set2.Test)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; p &lt;- with(Set2.Train, c(MAT, Precip, QUDO))</span><br />

&nbsp;

<br />
The results are plotted as a part of Fig. 7 below after first switching to a perspective more in line with ANN applications.

Specifically, the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">glm()</span> computes a logistic regression model based not on minimizing the error sum of squares but on a maximum likelihood procedure. As a transition from logistic regression to ANNs we will compute a least squares solution to Equation (1) using the R base function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span>. In this case, rather than being interpreted as a probability as in Equation (1), the logistic function is interpreted as the response of a cell such as those shown in Fig. 1. We will change the form of the logistic function to

<table style="width: 87.5967%;">
<tbody>
<tr>
<td style="width: 77.6%;" width="554">
                                               <img data-attachment-id="2046" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/log-2/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/log-2.jpg?fit=138%2C49&amp;ssl=1" data-orig-size="138,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="log 2" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/log-2.jpg?fit=138%2C49&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/log-2.jpg?fit=138%2C49&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/log-2.jpg?resize=138%2C49" alt="" class="alignnone size-full wp-image-2046" width="138" height="49" data-recalc-dims="1" />.
</td>
<td style="width: 9.8%;" width="70">
(2)
</td>
</tr>
</tbody>
</table>

Note that as <em>Z </em>increases, <img data-attachment-id="2048" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/phi-2/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?fit=34%2C25&amp;ssl=1" data-orig-size="34,25" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phi" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?fit=34%2C25&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?fit=34%2C25&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?resize=34%2C25" alt="" class="alignnone size-full wp-image-2048" width="34" height="25" data-recalc-dims="1" /> approaches 1, and as <em>Z </em>decreases toward minus infinity, <img data-attachment-id="2048" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/phi-2/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?fit=34%2C25&amp;ssl=1" data-orig-size="34,25" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phi" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?fit=34%2C25&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?fit=34%2C25&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/phi-1.jpg?resize=34%2C25" alt="" class="alignnone size-full wp-image-2048" width="34" height="25" data-recalc-dims="1" /> approaches 0. This form of the function is different from Equation (1) but it is an easy exercise to show that they are equivalent. The form of the function in Equation (2) follows that of Venables and Ripley (2002, p. 224) and is more common in the ANN literature.<br />
<br />
We will write the R code for the logistic function (this was used to plot Fig. 2) as follows.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; # Logistic function</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; lg.fn &lt;- function(coefs, a){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    b &lt;- coefs[1]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    w &lt;- coefs[2:length(coefs)]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    return(1 / (1 + exp(-(b + sum(w * a)))))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    }</span><br />
<br />

Here the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">b</span> corresponds to the bias terms <em>B</em> and the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">w</span> corresponds to the weights terms <em>W </em>in Fig. 1b. The <span style="font-size: 12pt; font-family: courier new, courier, monospace;">a</span> corresponds to the level of <em>activation</em> that the cell receives as input. Thus each cell receives an input activation <em>a</em> and produces an output activation <img data-attachment-id="2052" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/phia/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/phia.jpg?fit=35%2C22&amp;ssl=1" data-orig-size="35,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="phia" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/phia.jpg?fit=35%2C22&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/phia.jpg?fit=35%2C22&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/phia.jpg?resize=35%2C22" alt="" class="alignnone size-full wp-image-2052" width="35" height="22" data-recalc-dims="1" />.

Next we create a function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">SSE()</span> to compute the sum of squared errors. For simplicity of code we will restrict our models to the case of two predictors (i.e., <em>p</em> = 2).<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; SSE &lt;- function(w.io, X.Y, fn){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    n &lt;- length(X.Y) / 3</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    X1 &lt;- X.Y[1:n]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    X2 &lt;- X.Y[(n+1):(2<em>n)]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    Y &lt;- X.Y[(2</em>n+1):(3*n)]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    Yhat &lt;- numeric(n)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    for (i in 1:n) Yhat[i] &lt;- fn(w.io,<br />
+       c(X1[i], X2[i]))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    sse &lt;- 0</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    for (i in 1:n) sse &lt;- sse + (Y[i] &#8211; Yhat[i])^2</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    return(sse)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    }</span><br />
<br />

Here the argument <span style="font-size: 12pt; font-family: courier new, courier, monospace;">w.io</span> contains the biases and weights (the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">b</span> and <span style="font-size: 12pt; font-family: courier new, courier, monospace;">w</span> terms), which are the nonlinear regression coefficients, <span style="font-size: 12pt; font-family: courier new, courier, monospace;">X.Y</span> contains the <em>X<sub>i</sub></em> and <em>Y<sub>i</sub></em> terms, and <span style="font-size: 12pt; font-family: courier new, courier, monospace;">fn</span> is the function to compute (in this case <span style="font-size: 12pt; font-family: courier new, courier, monospace;">lg.fn()</span>). Here is the code to compute the least squares regression using <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span>, compare its coefficients and error sum of squares with that obtained from <span style="font-size: 12pt; font-family: courier new, courier, monospace;">glm()</span>, and draw Fig. 7. As with <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> above, the iteration in <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span> is started with a set of random numbers drawn from a unit normal distribution.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; print(coefs.glm &lt;- coef(mod.glm))</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">(Intercept)       MAT    Precip</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> -0.7704258   3.1803868  -5.3868261</span></strong><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; SSE(coefs.glm, p, lg.fn)</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">[1] 8.931932</span></strong><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; print(coefs.nlm &lt;- nlm(SSE, rnorm(3), p,<br />
+      lg.fn)$estimate)</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">[1] -0.9089667  3.3558542 -4.9486441</span></strong><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; SSE(coefs.nlm, p, lg.fn)</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">[1] 8.90217</span></strong><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; plot.ANN(Set2.Test, Predict.glm, 0.5, &#8220;Logistic<br />
+     Regression&#8221;) <em># Fig. 7</em></span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; with(Set2.Train, points(x = MAT, y = Precip,<br />
+  pch = 16,</span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> col = Color))  </span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; c &lt;- coefs.nlm</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; abline(-c[1]/c[3], -c[2]/c[3])</span><br />
<br />

<p style="text-align: left;"><br />
<img data-attachment-id="2053" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-7/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-7.jpg?fit=203%2C203&amp;ssl=1" data-orig-size="203,203" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 7" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-7.jpg?fit=203%2C203&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-7.jpg?fit=203%2C203&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-7.jpg?resize=203%2C203" alt="" class="size-full wp-image-2053 aligncenter" width="203" height="203" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-7.jpg?w=203&amp;ssl=1 203w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-7.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 203px) 100vw, 203px" data-recalc-dims="1" /><br />
<br />
Figure 7. Prediction zones computed using <span style="font-size: 12pt; font-family: courier new, courier, monospace;">glm()</span>. The black line shows the boundary of the prediction zones computed by minimizing the error sum of squares using <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span>.</p>

Now that we have a function to compute the logistic function and one to minimize the sum of squares, we are ready to construct a simple ANN with one hidden layer.

&nbsp;

<br />
<br />
Table 1. Notation used in the single hidden cell ANN.

<table>
<tbody>
<tr>
<td width="99">
Subscript or superscript
</td>
<td width="155">
Significance
</td>
<td width="133">
Range symbol
</td>
<td width="84">
Max value
</td>
</tr>
<tr>
<td width="99">
( )
</td>
<td width="155">
Layer <em>I, H, M</em>
</td>
<td width="133">
(1), (2), (3)
</td>
<td width="84">
(3)
</td>
</tr>
<tr>
<td width="99">
<em>i</em>
</td>
<td width="155">
Data record
</td>
<td width="133">
<em>n</em>
</td>
<td width="84">
50
</td>
</tr>
<tr>
<td width="99">
<em>j</em>
</td>
<td width="155">
Predictor
</td>
<td width="133">
<em>p</em>
</td>
<td width="84">
2
</td>
</tr>
<tr>
<td width="99">
<em>m</em>
</td>
<td width="155">
Hidden cell number
</td>
<td width="133">
<em>M</em>
</td>
<td width="84">
4
</td>
</tr>
</tbody>
</table>

&nbsp;

Table 1 shows the notation that we will use. There are <em>p</em> input cells at layer 1, numbered <em>1,&#8230;,p.</em> In our case, <em>p</em> = 2. Each input cell receives an input activation pair <em>X<sub>i,j</sub>, j = 1,…,n</em>. For the training data, <em>n</em> = 50. Fig. 8 shows a schematic of an ANN in which there is one hidden layer with four cells. As shown in the figure, the layers of cells are numbered in increasing order, so that the input layer is layer 1, the hidden layer is layer 2, and the output layer is layer 3. The layers are indicated by superscripts in parentheses. The connections between input cells and the hidden cells pass the weighted signal to the hidden layer cells. In Fig. 8 for each data record <em>i </em>each cell receives a signal consisting of the pair <em>X<sub>i,j</sub></em>, <em>j</em> = 1, 2, and passes it along to the hidden layer cells as an <em>activation</em> <img data-attachment-id="2039" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/eq-1/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-1.jpg?fit=90%2C28&amp;ssl=1" data-orig-size="90,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Eq 1" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-1.jpg?fit=90%2C28&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-1.jpg?fit=90%2C28&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-1.jpg?resize=90%2C28" alt="" class="alignnone size-full wp-image-2039" width="90" height="28" data-recalc-dims="1" />. Here we have to be a bit careful. The superscript (1) refers to the layer, <em>i </em>indexes the data record, which ranges from 1 to 50, <em>j </em>refers to the component of the predictor variable and has the value 1 or 2, and <em>m </em>indexes the hidden layer cell number and ranges from 1 to <em>M</em> = 4.

<img data-attachment-id="2054" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-8/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-8.jpg?fit=624%2C372&amp;ssl=1" data-orig-size="624,372" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 8" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-8.jpg?fit=450%2C268&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-8.jpg?fit=450%2C268&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-8.jpg?resize=450%2C268" alt="" class="size-medium wp-image-2054 aligncenter" width="450" height="268" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-8.jpg?resize=450%2C268&amp;ssl=1 450w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-8.jpg?w=624&amp;ssl=1 624w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />

Figure 8. Diagram of an ANN with one hidden layer having four cells.<br />
<br />


&nbsp;

The subscripts in Fig. 8 are the reverse of what one might expect in that information flows from input cell <em>j</em> in layer 1, the input layer, to cell <em>m</em> in layer 2, the hidden layer (for example, we write <img data-attachment-id="2040" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/a421/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a421.jpg?fit=28%2C28&amp;ssl=1" data-orig-size="28,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="a421" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a421.jpg?fit=28%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a421.jpg?fit=28%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a421.jpg?resize=28%2C28" alt="" class="alignnone size-full wp-image-2040" width="28" height="28" data-recalc-dims="1" />  instead of <img data-attachment-id="2041" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/a124/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a124.jpg?fit=28%2C28&amp;ssl=1" data-orig-size="28,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="a124" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a124.jpg?fit=28%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a124.jpg?fit=28%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/a124.jpg?resize=28%2C28" alt="" class="alignnone size-full wp-image-2041" width="28" height="28" data-recalc-dims="1" /> ; here <em>j</em> =2 identifies the input cell and <em>m</em> = 4 identifies the hidden layer cell). This seemingly backwards notation is traditional in the ANN literature and is used because the set of values is often represented as a matrix in which the values in the same layer form the rows. We will encounter this usage in Section 6.

The activation <img data-attachment-id="2055" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/amj/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/amj.jpg?fit=28%2C28&amp;ssl=1" data-orig-size="28,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="amj" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/amj.jpg?fit=28%2C28&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/amj.jpg?fit=28%2C28&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/amj.jpg?resize=28%2C28" alt="" class="alignnone size-full wp-image-2055" width="28" height="28" data-recalc-dims="1" />  is part of the information processed by hidden cell <em>m </em>in layer 2. The hidden cells respond to the sum of input activations. according to the equation

<table style="width: 96.5642%;">
<tbody>
<tr>
<td width="420">
                            <img data-attachment-id="2042" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/equ-3/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/equ-3.jpg?fit=268%2C28&amp;ssl=1" data-orig-size="268,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="equ 3" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/equ-3.jpg?fit=268%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/equ-3.jpg?fit=268%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/equ-3.jpg?resize=268%2C28" alt="" class="alignnone size-full wp-image-2042" width="268" height="28" data-recalc-dims="1" />.
</td>
<td width="52">
(3)
</td>
</tr>
</tbody>
</table>

This action takes place for each of the  data records. The one output cell (layer 3) receives the activation values  of each of the <em>M </em>hidden cells and processes these according to the equation

<table>
<tbody>
<tr>
<td width="429">
                 <img data-attachment-id="2056" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/eq4/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq4.jpg?fit=371%2C28&amp;ssl=1" data-orig-size="371,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Eq4" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq4.jpg?fit=371%2C28&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq4.jpg?fit=371%2C28&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq4.jpg?resize=371%2C28" alt="" class="alignnone size-full wp-image-2056" width="371" height="28" data-recalc-dims="1" />.
</td>
<td width="43">
(4)
</td>
</tr>
</tbody>
</table>

The quantity <em>Q<sub>i </sub></em>represents the estimate of <em>Y<sub>i</sub></em> given the current set of weights. Assuming that the sum of squared errors is used as the objective function, the value of the objective function

<table>
<tbody>
<tr>
<td width="414">
<p style="text-align: center;"><img data-attachment-id="2058" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/eq-5/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-5.jpg?fit=111%2C49&amp;ssl=1" data-orig-size="111,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Eq 5" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-5.jpg?fit=111%2C49&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-5.jpg?fit=111%2C49&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Eq-5.jpg?resize=111%2C49" alt="" class="alignnone size-full wp-image-2058" width="111" height="49" data-recalc-dims="1" /></p>
</td>
<td width="58">
(5)
</td>
</tr>
</tbody>
</table>

is computed. This quantity is then minimized to yield the final estimate . Table 2 summarizes this sequence of steps.

&nbsp;

<br />
Table 2 Sequence of steps in the computation of the ANN weights and biases.

<img data-attachment-id="2059" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/table-2/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-2.jpg?fit=1070%2C642&amp;ssl=1" data-orig-size="1070,642" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Table 2" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-2.jpg?fit=450%2C270&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-2.jpg?fit=450%2C270&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-2.jpg?resize=545%2C327" alt="" class="alignnone  wp-image-2059" width="545" height="327" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-2.jpg?resize=450%2C270&amp;ssl=1 450w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-2.jpg?resize=768%2C461&amp;ssl=1 768w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-2.jpg?w=1070&amp;ssl=1 1070w" sizes="(max-width: 545px) 100vw, 545px" data-recalc-dims="1" /><br />
<br />

We are now ready to start constructing our homemade ANN. First let’s see how many weights we will need. Fig. 8 indicates that if there are <em>M</em> hidden cells and two predictors then there will be 2<em>M</em> weights to cover the links from the input, plus <em>M</em> for the bias <img data-attachment-id="2060" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/b1/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?fit=28%2C22&amp;ssl=1" data-orig-size="28,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="b1" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?fit=28%2C22&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?fit=28%2C22&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?resize=28%2C22" alt="" class="alignnone size-full wp-image-2060" width="28" height="22" data-recalc-dims="1" />, for a total of 3<em>M. </em>From the hidden cells to the output cell there will be <em>M</em>, plus a final weight for the bias <img data-attachment-id="2061" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/b2/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" data-orig-size="28,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="b2" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?resize=28%2C22" alt="" class="alignnone size-full wp-image-2061" width="28" height="22" data-recalc-dims="1" /> for a grand total of 4<em>M + </em>1 weights. Recall that the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> reported 17 weights; this is where that number came from. We must pass all of the weights through the code as an array. Table 3 shows how the code of our homemade ANN arranges this array for an ANN with four hidden cells. The arrangement is organized to make the programming as simple as possible. Note, by the way, that the bias <img data-attachment-id="2060" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/b1/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?fit=28%2C22&amp;ssl=1" data-orig-size="28,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="b1" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?fit=28%2C22&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?fit=28%2C22&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/b1.jpg?resize=28%2C22" alt="" class="alignnone size-full wp-image-2060" width="28" height="22" data-recalc-dims="1" /> is applied to layer 2 and the bias <img data-attachment-id="2061" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/b2/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" data-orig-size="28,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="b2" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?resize=28%2C22" alt="" class="alignnone size-full wp-image-2061" width="28" height="22" data-recalc-dims="1" /> is applied to layer 3. This seems confusing, but again is be nearly universal practice, so I will go with it as well. Remember that  is the weight applied to the activation from the  cell in layer <em>l</em> applied to the  cell in layer <em>l </em>+ 1.<br />
<br />
Table 3. Arrangement of the 17 biases and weights for an ANN with four hidden cells.<br />
<img data-attachment-id="2063" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/table-3-2/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-3-1.jpg?fit=929%2C205&amp;ssl=1" data-orig-size="929,205" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Table 3" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-3-1.jpg?fit=450%2C99&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-3-1.jpg?fit=450%2C99&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-3-1.jpg?resize=541%2C119" alt="" class="alignnone  wp-image-2063" width="541" height="119" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-3-1.jpg?resize=450%2C99&amp;ssl=1 450w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-3-1.jpg?resize=768%2C169&amp;ssl=1 768w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Table-3-1.jpg?w=929&amp;ssl=1 929w" sizes="(max-width: 541px) 100vw, 541px" data-recalc-dims="1" />

We will first construct a prediction function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">pred.ann()</span> that, given a set of weights  and a pair of input values  for a fixed <em>i </em>calculates an estimate according to Equation (3).<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; pred.ANN1 &lt;- function(w.io, X){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     b.2 &lt;- w.io[1] <em># First beta is output bias</em></span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     b.w &lt;- w.io[-1] <em># Biases and weights</em></span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     M &lt;- length(b.w) / 4 # Number of hidden cells</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><em># vector to hold hidden cell output activation</em><br />
+     a.2 &lt;- numeric(M)<br />
<em># Hidden cell biases and weights </em></span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     w.2 &lt;- b.w[(3 * M + 1):(4 * M)] </span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     for (m in 1:M){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+        bw.1 &lt;- <br />
+           b.w[(1 + (m &#8211; 1) * 3):(3 + (m &#8211; 1) * 3)]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+        a.2[m] &lt;- lg.fn(bw.1, X)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+        }</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     phi.out &lt;- lg.fn(c(b.2, w.2), a.2)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     return(phi.out)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+     }</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">}</span><br />
<br />
The arguments <span style="font-size: 12pt; font-family: courier new, courier, monospace;">w.io</span> and <span style="font-size: 12pt; font-family: courier new, courier, monospace;">X</span> contain the biases and weights and the predictor values <em>X</em>, respectively. The first two lines extract the output cell bias <span style="font-size: 12pt; font-family: courier new, courier, monospace;">b.2</span>, which corresponds to <img data-attachment-id="2061" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/b2/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" data-orig-size="28,22" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="b2" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?fit=28%2C22&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/b2.jpg?resize=28%2C22" alt="" class="alignnone size-full wp-image-2061" width="28" height="22" data-recalc-dims="1" /> in Table 3, and the third lines determines the number of hidden cells by dividing the number of weights by 4. The next three lines establish locations to hold the values of the quantities in Table 3. The remaining lines of code compute the quantity <em>Q<sub>i</sub></em> in Equation (4).<br />
<br />

The next function generates the ANN itself, which we call <span style="font-size: 12pt; font-family: courier new, courier, monospace;">ANN.1()</span>.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; ANN.1 &lt;- function(X, Y, M, fn, niter){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    w.io &lt;- rnorm(4*M + 1)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    X.Y &lt;- c(as.vector(X), Y)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    return(nlm(SSE, w.io, X.Y, fn, iterlim = niter))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    } </span><br />
<br />

The arguments are respectively <span style="font-size: 12pt; font-family: courier new, courier, monospace;">X</span>, a matrix or data frame whose columns are the <span style="font-size: 12pt; font-family: courier new, courier, monospace;"><em>X<sub>i,j</sub></em></span>, <span style="font-size: 12pt; font-family: courier new, courier, monospace;">Y</span>, the vector of <em>Y</em> values, <span style="font-size: 12pt; font-family: courier new, courier, monospace;">M</span>, the number of hidden cells, <span style="font-size: 12pt; font-family: courier new, courier, monospace;">fn</span>, the function defining the cells’ response, and <span style="font-size: 12pt; font-family: courier new, courier, monospace;">niter</span>, the maximum allowable number of iterations. The first line in the function generates the random starting weights and the second line arranges the data as a single column. The function then calls <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span> to carry out the nonlinear minimization of the sum of squared errors. There are other error measures beside the sum of squared errors that we could use, and some will be discussed later.

Now we must set the quantities to feed to the ANN and pass them along.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; set.seed(1)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; X &lt;- with(Set2.Train, (c(MAT, Precip)))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; mod.ANN1 &lt;- ANN.1(X, Set2.Train$QUDO, 4, pred.ANN,<br />
+    1000)</span>

&nbsp;

<br />
The function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">ANN.1()</span> returns the full output of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span>, which includes the coefficients as well as a code indicating the conditions under which the function terminated. There are five possible values of this code; you can use <span style="font-size: 12pt; font-family: courier new, courier, monospace;">?nlm</span> to see what they signify. One point, however, must be noted. The function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span> makes use of Newton’s method to estimate the minimum of a function. The specific algorithm is described by Schnabel et al. (1985). The important point is that the method makes use of the <em>Hessian</em> matrix, which is the matrix of second derivatives. If there are thousands of weights, as there might be in an ANN, this is an enormous matrix. For this reason, newer ANN algorithms generally use the <em>gradient descent</em> method, for which only first derivatives are needed. We will return to this issue in Section 3.

Here is the output from <span style="font-size: 12pt; font-family: courier new, courier, monospace;">ANN.1()</span>. Again, your output will probably be different from mine.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; print(beta.ann &lt;- mod.ANN1$estimate)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong> [1]  191.48348  198.97750 -407.10859  269.84118 -824.73518<br />
[6]  898.75456  826.91247  473.50764 -469.37435 -241.65738<br />
[11]  443.55838 -201.72335 -149.85410 -188.09470 -168.33946<br />
[16]  -96.12398   73.12316<br />
</strong>&gt; print(code.ann &lt;- mod.ANN1$code)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>[1] 3</strong></span>

&nbsp;

<br />
The code value 3 indicates that at least a local minimum was probably found. The output is plotted in Fig. 9.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; Set2.Test$QUDO &lt;- 0</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; for (i in 1:length(Set2.Test$QUDO))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    Set2.Test$QUDO[i] &lt;- pred.ANN1(beta.ann,<br />
+        c(Set2.Test$MAT[i],</span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> Set2.Test$Precip[i]))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; plot.ANN(Set2.Test, Set2.Test$QUDO, 0.5,<br />
+    &#8220;ANN.1 Predictions&#8221;) </span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; with(Set2.Train, points(x = MAT, y = Precip,<br />
+    pch = 16,</span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> col = Color)) <em># Fig. 9</em> </span>

&nbsp;

<br />
Similar to Fig. 6, this figure shows a “bestiary” of predictions of this artificial neural network. This was created by sequencing the random number seed through integer values starting at 1 and picking out results that looked interesting.<br />
<br />


<img data-attachment-id="2064" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-9/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-9.jpg?fit=412%2C270&amp;ssl=1" data-orig-size="412,270" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 9" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-9.jpg?fit=412%2C270&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-9.jpg?fit=412%2C270&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-9.jpg?resize=412%2C270" alt="" class="alignnone size-full wp-image-2064 aligncenter" width="412" height="270" data-recalc-dims="1" />

&nbsp;

Figure 9. A “bestiary” of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">ANN.1()</span> predictions. As with <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span>, most of the time the output is relatively tame, but sometimes it is not.

&nbsp;

<p style="text-align: left;">In order to examine more closely the workings of the ANN we will focus on the lower middle prediction in Fig. 9, which was created using the random number seed set to 7, and examine this for a fixed value of the predictor <em>Precip</em>. Fig. 10 plots the ANN output with the set of values <em>Precip</em> = 0.8 highlighted.<br />
<br />
<br />
<br />
<img data-attachment-id="2065" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-10/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-10.jpg?fit=148%2C148&amp;ssl=1" data-orig-size="148,148" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 10" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-10.jpg?fit=148%2C148&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-10.jpg?fit=148%2C148&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-10.jpg?resize=148%2C148" alt="" class="size-full wp-image-2065 aligncenter" width="148" height="148" data-recalc-dims="1" /></p>

Figure 10. Replot of the sixth output displayed in Figure 8 with values <em>Precip</em> = 0.8 highlighted.<br />
<br />


&nbsp;

<p style="text-align: center;"><img data-attachment-id="2066" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-11/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-11.jpg?fit=395%2C396&amp;ssl=1" data-orig-size="395,396" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 11" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-11.jpg?fit=299%2C300&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-11.jpg?fit=395%2C396&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-11.jpg?resize=299%2C300" alt="" class="alignnone size-medium wp-image-2066" width="299" height="300" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-11.jpg?resize=299%2C300&amp;ssl=1 299w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-11.jpg?resize=150%2C150&amp;ssl=1 150w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-11.jpg?w=395&amp;ssl=1 395w" sizes="(max-width: 299px) 100vw, 299px" data-recalc-dims="1" /></p>

Figure 11. Activation  of each individual hidden cell.<br />
<br />


Fig. 11 shows the hidden cell activation  together with the value of the weight  multiplying this response in Equation (3). Note that the individual cells can be said to “organize” themselves to respond in different ways to the inputs. For this reason during the heady days of early work in ANNs systems like this were sometimes called “self-organizing systems.” Also, the first two cells appear to have similar response, although, as can be seen by inspecting the bias and weight values in the output of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">ANN.1()</span> above (with reference to Table 1), these values are actually very different.

Fig. 12 is a plot along the highlighted line <em>Precip = </em>0.8 of the same output as displayed in Fig. 11.

<img data-attachment-id="2067" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-12/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-12.jpg?fit=410%2C400&amp;ssl=1" data-orig-size="410,400" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 12" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-12.jpg?fit=308%2C300&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-12.jpg?fit=410%2C400&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-12.jpg?resize=308%2C300" alt="" class="alignnone size-medium wp-image-2067 aligncenter" width="308" height="300" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-12.jpg?resize=308%2C300&amp;ssl=1 308w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-12.jpg?w=410&amp;ssl=1 410w" sizes="(max-width: 308px) 100vw, 308px" data-recalc-dims="1" />

&nbsp;

Figure 12. Plot of the hidden cell activation  <img data-attachment-id="2068" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/am2-form/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/am2-form.jpg?fit=166%2C28&amp;ssl=1" data-orig-size="166,28" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="am2 form" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/am2-form.jpg?fit=166%2C28&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/am2-form.jpg?fit=166%2C28&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/am2-form.jpg?resize=166%2C28" alt="" class="alignnone size-medium wp-image-2068" width="166" height="28" data-recalc-dims="1" /> along the highlighted line in Fig. 10.

&nbsp;

Figure 13 shows the weighted activation values  of the hidden cells along the row of highlighted values. Note the different abscissa ranges of the plots in Figs. 12 and 13.

<img data-attachment-id="2069" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-13/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-13.jpg?fit=444%2C438&amp;ssl=1" data-orig-size="444,438" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 13" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-13.jpg?fit=304%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-13.jpg?fit=444%2C438&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-13.jpg?resize=304%2C300" alt="" class="alignnone size-medium wp-image-2069 aligncenter" width="304" height="300" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-13.jpg?resize=304%2C300&amp;ssl=1 304w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-13.jpg?w=444&amp;ssl=1 444w" sizes="(max-width: 304px) 100vw, 304px" data-recalc-dims="1" />

Figure 13. Plot of the weighted hidden cell activation  along the highlighted line in Fig. 10.

Fig. 14a shows the sum passed to the output cell, given by  in Equation (3), and Fig. 14b shows the sum  when the bias  is added. Finally, Fig. 14c shows the output value .

<br />
<img data-attachment-id="2070" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-14/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-14.jpg?fit=616%2C205&amp;ssl=1" data-orig-size="616,205" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 14" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-14.jpg?fit=450%2C150&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-14.jpg?fit=450%2C150&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-14.jpg?resize=543%2C181" alt="" class="alignnone  wp-image-2070" width="543" height="181" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-14.jpg?resize=450%2C150&amp;ssl=1 450w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-14.jpg?w=616&amp;ssl=1 616w" sizes="(max-width: 543px) 100vw, 543px" data-recalc-dims="1" />

               (a)                                       (b)                                       (c)

Figure 14. Referring to Equation (3): (a) sum  of inputs to the output cell; (b) bias added to sum of weighted inputs, ; (c) final output.

The values of the first cell are universally low, so it apparently plays almost no role in the final result, we might be able achieve some simplification by pruning it. Let’s check it out.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; Set2.Test$QUDO1 &lt;- 0</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; beta.ann1 &lt;- beta.ann[c(1,5:16)]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; for (i in 1:length(Set2.Test$QUDO))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    Set2.Test$QUDO1[i] &lt;- pred.ANN1(beta.ann1, c(Set2.Test$MAT[i],</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+         Set2.Test$Precip[i]))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; max(abs(Set2.Test$QUDO &#8211; Set2.Test$QUDO1))  </span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">[1] 1</span><br />
<br />

<br />
Obviously, cell 1 does play an important role for some values of the predictors. Sometimes a cell can be pruned in this way without changing the results and sometimes not. Algorithms do exist for determining whether individual cells in an ANN can be pruned (Bishop, 1995). We will see in Section 6 that examination of the contributions of cells can also be used in variable selection.

We mentioned earlier that the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nlm()</span>, used in our function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">SSE()</span> to locate the minimum of the sum of squared errors, could be consumptive of time and memory for large problems. Ripley (1996, p. 158), writing at a time when computers were much slower than today, asserted that any ANN with fewer than around 1,000 weights could be solved effectively using Newton’s method. The function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> uses an approximation of Newton’s method called the BGFS method (see <span style="font-size: 12pt; font-family: courier new, courier, monospace;">?nnet</span> and <span style="font-size: 12pt; font-family: courier new, courier, monospace;">?optim</span>). We can to some extent test Newton’s method by expanding the size of our ANN to ten hidden cells and running it on the full data set of 2267 records.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; set.seed(3)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; X &lt;- with(data.Set2U, (c(MAT, Precip)))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; start_time &lt;- Sys.time()</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; mod.ANN1 &lt;- ANN.1(X, data.Set2U$QUDO, 10,<br />
+     pred.ANN1, 1000)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; print(code.ann &lt;- mod.ANN1$code)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"></span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"></span><strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">[1] 1</span><br />
</strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; stop_time &lt;- Sys.time()</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; stop_time &#8211; start_time</span><strong><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">Time difference of 9.562427 secs</span><br />
<br />
<br />
</strong>

The return code of 1 indicates iteration to convergence. When using other random seeds the program occasionally goes down a rabbit hole and has to be manually stopped by hitting the &lt;esc&gt; key. When it does converge, it usually doesn’t take too long.

We now return to the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet</span> package and the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> function, which we will apply to the entire data set. First, however, we need to discuss the use of this function for classification as opposed to regression. Our use of the numerical values 0 and 1 for the class label <span style="font-size: 12pt; font-family: courier new, courier, monospace;">QUDO</span> makes <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> treat this as a regression problem. We can change to a classification problem easily by defining a factor valued data field and using this in the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> model. This seemingly small change, however, gives us the opportunity to look more closely at how the ANN computes its values and how error is interpreted. As with logistic regression, when <span style="font-size: 12pt; font-family: courier new, courier, monospace;">QUDO</span> is an integer quantity, the estimate returns values on the interval (0,1) (i.e., all numbers greater than 0 and less than 1). The use of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> in classification is discussed by Venables and Ripley (2002, p. 342). We will define a new quantity <span style="font-size: 12pt; font-family: courier new, courier, monospace;">QUDOF</span> as a factor taking on the factor values “0” and “1”.<br />
<br />
<br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; data.Set2U$QUDOF &lt;- as.factor(data.Set2U$QUDO)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; unique(data.Set2U$QUDOF)</span><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">[1] 1 0</span></strong><br />
<strong><span style="font-size: 12pt; font-family: courier new, courier, monospace;">Levels: 0 1</span></strong><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">modf.nnet &lt;- nnet(QUDOF ~ MAT + Precip,<br />
+    data = data.Set2U,</span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> size = 4, maxit = 1000)</span>

<br />

Let’s take a closer look at the predicted values<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; hist(Predict.nnet, breaks = seq(0, 1, 0.1),</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+   main = &#8220;Histogram of nnet Predictions&#8221;) <em>#Fig. 15<br />
<br />
</em></span>

<img data-attachment-id="2071" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-15/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-15.jpg?fit=292%2C292&amp;ssl=1" data-orig-size="292,292" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 15" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-15.jpg?fit=292%2C292&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-15.jpg?fit=292%2C292&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-15.jpg?resize=292%2C292" alt="" class="alignnone size-full wp-image-2071 aligncenter" width="292" height="292" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-15.jpg?w=292&amp;ssl=1 292w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-15.jpg?resize=150%2C150&amp;ssl=1 150w" sizes="(max-width: 292px) 100vw, 292px" data-recalc-dims="1" />

Figure 15.  Histogram of predicted values of the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span>.

&nbsp;

As shown in Fig. 15, although <span style="font-size: 12pt; font-family: courier new, courier, monospace;">QUDOF </span>is a nominal scale quantity (see SDA2, Sec. 4.2.1), the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> still returns values on the interval (0,1). Unlike the case with logistic regression, these cannot be interpreted as probabilities, but we can interpret the output as an indication of the “strength” of the classification. Since the returned values are distributed all along the range from 0 to 1, it makes sense to establish the cutoff point using a ROC curve. We will use the functions of the package <span style="font-size: 12pt; font-family: courier new, courier, monospace;">ROCR</span> to do so.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; library(ROCR)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; pred.QUDO &lt;- predict(modf.nnet, data.Set2U)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; pred &lt;- prediction(pred.QUDO, data.Set2U$QUDO)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; perf &lt;- performance(pred, &#8220;tpr&#8221;, &#8220;fpr&#8221;)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; plot(perf, colorize = TRUE,<br />
+      main = &#8220;nnet ROC Curve&#8221;) <em># Fig. 16a<br />
</em>&gt; perf2 &lt;- performance(pred, &#8220;acc&#8221;)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; plot(perf2, main = <br />
+   &#8220;nnet Prediction Accuracy&#8221;)  <em># Fig 16b<br />
</em>&gt; print(best.cut &lt;- which.max(perf2@y.values[[1]]))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>[1] 565<br />
</strong>&gt; print(cutoff &lt;- perf2@x.values[[1]][best.cut])</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>[1] 0.4547653</strong></span><strong> <br />
</strong>

       <img data-attachment-id="2072" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-16/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-16.jpg?fit=557%2C275&amp;ssl=1" data-orig-size="557,275" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 16" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-16.jpg?fit=450%2C222&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-16.jpg?fit=450%2C222&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-16.jpg?resize=450%2C222" alt="" class="alignnone size-medium wp-image-2072" width="450" height="222" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-16.jpg?resize=450%2C222&amp;ssl=1 450w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-16.jpg?w=557&amp;ssl=1 557w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />

                            (a)                                                  (b)<br />
Figure 16. (a) ROC curve for the nnet() prediction of <em>QUDO</em> presence/absence; (b) plot of prediction accuracy against cutoff value.

The ROC curve analysis indicates that the cutoff value leading to the highest prediction accuracy is around 0.45. Fig. 17a shows a replot of the full data set plotted in Fig. 3a and Fig. 17b shows a plot of the prediction regions.

                   <img data-attachment-id="2073" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-17/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-17.jpg?fit=343%2C164&amp;ssl=1" data-orig-size="343,164" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 17" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-17.jpg?fit=343%2C164&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-17.jpg?fit=343%2C164&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-17.jpg?resize=416%2C199" alt="" class="alignnone  wp-image-2073" width="416" height="199" data-recalc-dims="1" /><br />
                                      (a)                                              (b)

Figure 17.  (a) Plot of the QUDO data; (b) plot of the prediction regions.

The ROC curve can be used to compute a widely used performance measure, the area under the ROC curve, commonly known as the AUC. Looking at Fig. 16a, one can see that if the predictor provides a perfect prediction (zero false positives or false negatives) then the AUC will be one, and if the predictor is no better than flipping a coin then the AUC will be one half. The Wikipedia article on the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC curve</a> provides a thorough discussion of the AUC. Here is the calculation for the current case.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; auc.perf &lt;- performance(pred,&#8221;auc&#8221;)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; print(auc &lt;- as.numeric(auc.perf@y.values))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>[1] 0.8872358<br />
<br />
</strong></span>

<strong> </strong>Returning to the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> function, another feature of this function when the class labels are factors is its default error measure (this is discussed in the <em>Details</em> section of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">?nnet</span>). We can see this by applying the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">str()</span> function to the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> output.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; str(modf.nnet)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>List of 19<br />
  *    *    Deleted    *    *<br />
$ entropy      : logi TRUE<br />
  *    *    Deleted    *    *</strong></span>

<br />
The argument <span style="font-size: 12pt; font-family: courier new, courier, monospace;">entropy </span>is by default set to <span style="font-size: 12pt; font-family: courier new, courier, monospace;">TRUE</span>. If you try this with the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet </span>object created earlier with <span style="font-size: 12pt; font-family: courier new, courier, monospace;">QUDO</span> taking on integer values, you will see that the default value of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">entropy </span>in this case is <span style="font-size: 12pt; font-family: courier new, courier, monospace;">FALSE</span> (the value of <span style="font-size: 12pt; font-family: courier new, courier, monospace;">entropy </span>can be controlled in <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span> via the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">entropy</span> argument). When <span style="font-size: 12pt; font-family: courier new, courier, monospace;">entropy </span>is <span style="font-size: 12pt; font-family: courier new, courier, monospace;">TRUE</span>, the prediction error is not measured by the sum of squared errors as defined in Equation (5). Instead, it is measured by the <em>cross entropy</em>. For a factor valued quantity we can define this as (Guenther and Frisch, 2010, Venables and Ripley, 2002, p. 245)

<table>
<tbody>
<tr>
<td width="413">
                                                 <img data-attachment-id="2074" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/crossent/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/crossent.jpg?fit=104%2C49&amp;ssl=1" data-orig-size="104,49" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="crossent" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/crossent.jpg?fit=104%2C49&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/crossent.jpg?fit=104%2C49&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/crossent.jpg?resize=104%2C49" alt="" class="alignnone size-full wp-image-2074" width="104" height="49" data-recalc-dims="1" />,
</td>
<td width="59">
(6)
</td>
</tr>
</tbody>
</table>

where <em>t<sub>i</sub></em> is 1 for those values of <em>Q<sub>i</sub></em> that do not match the corresponding <em>Y<sub>i</sub></em>, and 0 for those that do, and <em>a<sub>i</sub></em> is a measure of the activity of the output cell. There is some evidence (e.g., Du and Swamy 2014, p. 37) that this measure avoids the creation of “flat spots” in the surface (or hypersurface) over which the optimization is being computed. Fig. 18 gives a sense of the difference. We will meet the cross entropy again in Section 3.<br />
<br />
                           <img data-attachment-id="2075" data-permalink="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/fig-18/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-18.jpg?fit=320%2C154&amp;ssl=1" data-orig-size="320,154" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig 18" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-18.jpg?fit=320%2C154&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-18.jpg?fit=320%2C154&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/09/Fig-18.jpg?resize=320%2C154" alt="" class="alignnone size-full wp-image-2075" width="320" height="154" data-recalc-dims="1" />

Figure 18. Plots of the sum of squared errors and the cross entropy for a range of error measures between 0 and 1.

A potentially important issue in the development of an ANN is the number of cells. The <span style="font-size: 12pt; font-family: courier new, courier, monospace;">tune()</span> function of the <span style="font-size: 12pt; font-family: courier new, courier, monospace;">e1071</span> package (Meyer et al., 2019), discussed in the <a href="https://psfaculty.plantsciences.ucdavis.edu/plant/additionaltopics_svm.pdf">Additional Topic on Support Vector Machines</a>, has an implementation <span style="font-size: 12pt; font-family: courier new, courier, monospace;">tune.nnet()</span>. The primary intended use of the function is for cross-validation, which will be discussed below. The function can, however, also be used to rapidly assess the effect of increasing the number of hidden cells.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; library(e1071)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; tune.nnet(QUDOF ~ MAT + Precip, data = data.Set2U,</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    size = 4)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>Error estimation of ‘nnet’ using 10-fold cross validation: 0.2056282<br />
Time difference of 9.346614 secs<br />
</strong>&gt; tune.nnet(QUDOF ~ MAT + Precip, data = data.Set2U,</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    size = 8)</span>

<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>Error estimation of ‘nnet’ using 10-fold cross validation: 0.2024174<br />
Time difference of 15.9804 secs<br />
</strong>&gt; tune.nnet(QUDOF ~ MAT + Precip, data = data.Set2U,</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    size = 12)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>Error estimation of ‘nnet’ using 10-fold cross validation: 0.1943285<br />
Time difference of 22.1304 secs<br />
<br />
</strong></span>

<strong> </strong>The code to calculate the elapsed time is not shown. These results provide evidence that for this particular problem there is only a very limited advantage to increasing the number of cells.

We can use repeated evaluation to pick the prediction that provides the best fit. We will try twenty different random starting configurations to see the distribution of error rates (keeping the same cutoff value for all of them).<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; seeds &lt;- numeric(20)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; error.rate &lt;- numeric(length(seeds))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; conv &lt;- numeric(length(seeds))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; seed &lt;- 0</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; i &lt;- 1</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; data.set &lt;- Set2.Test</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; while (i &lt;= length(seeds)){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    seed &lt;- seed + 1</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    set.seed(seed)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    md &lt;- nnet(QUDO ~ MAT + Precip,<br />
+      data = data.Set2U,</span><span style="font-size: 12pt; font-family: courier new, courier, monospace;"> size = 4, maxit = 1000,<br />
+      trace = FALSE)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    if (md$convergence == 0){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       seeds[i] &lt;- seed</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       pred &lt;- predict(md)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       QP &lt;- numeric(length(pred))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       QP[which(pred &gt; cutoff)] &lt;- as.factor(1)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       error.rate[i] &lt;-</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+          length(which(QP != data.Set2U$QUDOF)) /<br />
+              nrow(data.Set2U)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       i &lt;- i + 1</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       }</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    } </span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; seeds</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong> [1]  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21<br />
</strong>&gt; sort(error.rate)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong> [1] 0.1782091 0.1804146 0.1817380 0.1821791 0.1830613 0.1839435 0.1843846<br />
[8] 0.1843846 0.1848258 0.1857080 0.1865902 0.1874724 0.1887958 0.1905602<br />
[15] 0.1932069 0.1954124 0.1971769 0.2015880 0.2037936 0.2042347<br />
</strong>&gt; which.min(error.rate)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>[1] 17</strong></span>

<strong> </strong>

All but one of the seeds converged, and the best error rate is obtained using the seventeenth random number seed. The error rate here is not the same as the cross-validation error rate calculated above, as reflected in the different values. A plot of the predictions using this seed is similar to that shown in Fig. 16b.

Looking at that figure, one might suspect that the model may overfit the data. The concept of overfitting and underfitting in terms of the <em>bias-variance tradeoff</em> is discussed in a linear regression context in SDA2 Section 8.2.1 and in a classification context in the <a href="https://psfaculty.plantsciences.ucdavis.edu/plant/additionaltopics_svm.pdf">Additional Topic on Support Vector Machines</a>. In summary, a model that fits a training data set too closely (overfits) may do a worse job of fitting a different data set and, as discussed in the two sources, has a high variance. A simpler model that does not fit the training data as well has a higher bias, and the decision of which model to choose is a matter of balancing these two factors. Ten-fold cross validation is commonly used to test where a model stands the bias-variance tradeoff (again see the two sources given above). The following code carries out such a cross-validation.<br />
<br />


<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; best.seed &lt;- seeds[which.min(error.rate)]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; ntest &lt;- trunc(nrow(data.Set2U) / 10) * 10</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; data.set &lt;- data.Set2U[1:ntest,]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; n.rand &lt;- sample(1:nrow(data.set))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; n &lt;- nrow(data.set) / 10</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; folds &lt;- matrix(n.rand, nrow = n, ncol = 10)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; SSE &lt;- 0</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; for (i in 1:10){</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    test.fold &lt;- i</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    if (i == 1) train.fold &lt;- 2:10</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    if (i == 10) train.fold &lt;- 1:9</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    if ((i &gt; 1) &amp; (i &lt; 10)) train.fold &lt;-<br />
+         c(1:(i-1), (i+1):10)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    train &lt;- <br />
+       data.frame(data.set[folds[,train.fold],])</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    test &lt;-<br />
+         data.frame(data.set[folds[,test.fold],])</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    true.sp &lt;- data.set$QUDO[folds[,test.fold]]</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    set.seed(best.seed)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    modf.nnet &lt;- nnet(QUDOF ~ MAT + Precip,</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+       data = train, size = 8, maxit = 10000,<br />
+           Trace = FALSE)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    pred.1 &lt;- predict(modf.nnet, test)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    test$QUDO &lt;- 0</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    test$QUDO[which(pred.1 &gt;= 0.52)] &lt;- 1</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    E &lt;- length(which(test$QUDO != true.sp)) /<br />
+        length(true.sp)</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    SSE &lt;- SSE + E^2</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">+    }</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;">&gt; print(RMSE &lt;- sqrt(SSE / 10))</span><br />
<span style="font-size: 12pt; font-family: courier new, courier, monospace;"><strong>[1] 0.1930847</strong></span>

&nbsp;

The RMSE seems aligned with the error rate.

Venables and Ripley (2002) and Ripley (1996) discuss some of the options available with the function <span style="font-size: 12pt; font-family: courier new, courier, monospace;">nnet()</span>. Some of these are explored in Exercises (4) and (5). Now that we have some experience with an ANN having a single hidden layer, we will move on to the package <span style="font-size: 12pt; font-family: courier new, courier, monospace;">neuralnet</span>, in which multiple hidden layers are possible.

This concludes the first installment. Part 2 will be published subsequently. Again, R code and data are available in the Additional Topics section of SDA2, <a href="http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm">http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm</a>

<strong> </strong><hr style="border-top:black solid 1px" /><a href="https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/">Spatial Data Analysis Using Artificial Neural Networks Part 1</a> was first posted on October 9, 2020 at 8:52 am.<br />]]></content:encoded>
					
					<wfw:commentRss>https://r-posts.com/spatial-data-analysis-using-artificial-neural-networks-part-1/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Analyzing Remote Sensing Data using Image Segmentation</title>
		<link>https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/</link>
					<comments>https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/#comments</comments>
		
		<dc:creator><![CDATA[Richard Plant]]></dc:creator>
		<pubDate>Mon, 30 Mar 2020 17:45:01 +0000</pubDate>
				<category><![CDATA[R]]></category>
		<guid isPermaLink="false">http://r-posts.com/?p=1383</guid>

					<description><![CDATA[This post summarizes material posted as an Additional Topic to accompany the book Spatial Data Analysis in Ecology and Agriculture using R, Second Edition. The full post, together with R code and data, can be found in the Additional Topics section of the book’s website, http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm 1. Introduction The idea is best described with images. &#8230; <a href="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/" class="more-link">Continue reading <span class="screen-reader-text">Analyzing Remote Sensing Data using Image Segmentation</span></a><hr style="border-top:black solid 1px" /><a href="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/">Analyzing Remote Sensing Data using Image Segmentation</a> was first posted on March 30, 2020 at 5:45 pm.<br />]]></description>
										<content:encoded><![CDATA[This post summarizes material posted as an Additional Topic to accompany the book <em>Spatial Data Analysis in Ecology and Agriculture using R, Second Edition</em>. The full post, together with R code and data, can be found in the Additional Topics section of the book’s website, <a href="http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm">http://psfaculty.plantsciences.ucdavis.edu/plant/sda2.htm</a><br />
<br />
<em><strong>1. Introduction<br />
</strong></em>The idea is best described with images. Here is a display in <span style="font-family: courier new, courier, monospace">mapview</span> of an agricultural region in California’s Central Valley. The boundary of the region is a rectangle developed from <a href="https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system">UTM coordinates</a>, and it is interesting that the boundary of the region is slightly tilted with respect to the network of roads and field boundaries. There are several possible explanations for this, but it does not affect our presentation, so we will ignore it.<br />
<img data-attachment-id="1386" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/fig1-3/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig1.jpg?fit=454%2C261&amp;ssl=1" data-orig-size="454,261" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig1" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig1.jpg?fit=450%2C259&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig1.jpg?fit=450%2C259&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig1.jpg?resize=450%2C259" alt="" class="size-medium wp-image-1386 aligncenter" width="450" height="259" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig1.jpg?resize=450%2C259&amp;ssl=1 450w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig1.jpg?w=454&amp;ssl=1 454w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />There are clearly several different cover types in the region, including fallow fields, planted fields, natural regions, and open water. Our objective is to develop a land classification of the region based on Landsat data. A common method of doing this is based on the so-called <em>false color</em> image. In a false color image, radiation in the green wavelengths is displayed as blue, radiation in the red wavelengths is displayed as green, and radiation in the infrared wavelengths is displayed as red. Here is a false color image of the region shown in the black rectangle.  <br />
<img data-attachment-id="1387" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/fig2-2/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig2.jpg?fit=392%2C223&amp;ssl=1" data-orig-size="392,223" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig2" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig2.jpg?fit=392%2C223&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig2.jpg?fit=392%2C223&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig2.jpg?resize=392%2C223" alt="" class="size-full wp-image-1387 aligncenter" width="392" height="223" data-recalc-dims="1" /></p>

Because living vegetation strongly reflects radiation in the infrared region and absorbs it in the red region, the amount of vegetation contained in a pixel can be estimated by using the <em>normalized difference vegetation index</em>, or NDVI, defined as

<table>
<tbody>
<tr>
<td style="text-align: center" width="548"><img data-attachment-id="1388" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/ndvi/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/NDVI.jpg?fit=247%2C96&amp;ssl=1" data-orig-size="247,96" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="NDVI" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/NDVI.jpg?fit=247%2C96&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/NDVI.jpg?fit=247%2C96&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/NDVI.jpg?resize=187%2C73" alt="" class="alignnone  wp-image-1388" width="187" height="73" data-recalc-dims="1" /></td>
<td width="76"></td>
</tr>
</tbody>
</table>

where <em>IR </em>is the pixel intensity of the infrared band and <em>R</em> is the pixel intensity of the red band. Here is the NDVI of the region in the false color image above.<br />
<img data-attachment-id="1389" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/fig3/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig3.jpg?fit=357%2C357&amp;ssl=1" data-orig-size="357,357" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="Fig3" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig3.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig3.jpg?fit=357%2C357&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig3.jpg?resize=300%2C300" alt="" class="alignnone size-medium wp-image-1389 aligncenter" width="300" height="300" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig3.jpg?resize=300%2C300&amp;ssl=1 300w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig3.jpg?resize=150%2C150&amp;ssl=1 150w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/Fig3.jpg?w=357&amp;ssl=1 357w" sizes="(max-width: 300px) 100vw, 300px" data-recalc-dims="1" />If you compare the images you can see that the areas in  black rectangle above that appear green have a high NDVI, the areas that appear brown have a low NDVI, and the open water in the southeast corner of the image actually has a negative NDVI, because water strongly absorbs infrared radiation. Our objective is to generate a classification of the land surface into one of five classes: dense vegetation, sparse vegetation, scrub, open land, and open water. Here is the classification generated using the method I am going to describe.<br />
<br />


<img data-attachment-id="1394" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/landclasses/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/LandClasses.jpg?fit=624%2C325&amp;ssl=1" data-orig-size="624,325" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="LandClasses" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/LandClasses.jpg?fit=450%2C234&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/LandClasses.jpg?fit=450%2C234&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/LandClasses.jpg?resize=450%2C234" alt="" class="size-medium wp-image-1394 aligncenter" width="450" height="234" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/LandClasses.jpg?resize=450%2C234&amp;ssl=1 450w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/LandClasses.jpg?w=624&amp;ssl=1 624w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />

There are a number of image analysis packages available in R. I elected to use the <span style="font-family: courier new, courier, monospace">OpenImageR</span> and <span style="font-family: courier new, courier, monospace">SuperpixelImageSegmentation</span> packages, both of which were developed by L. Mouselimis (2018, 2019a). One reason for this choice is that the objects created by these packages have a very simple and transparent data structure that makes them ideal for data analysis and, in particular, for integration with the <span style="font-family: courier new, courier, monospace">raster</span> package.  The packages are both based on the idea of image segmentation using <em>superpixels </em>. According to Stutz et al. (2018), superpixels were introduced by Ren and Malik (2003). Stutz et al. define a superpixel as “a group of pixels that are similar to each other in color and other low level properties.” Here is the false color image above divided into superpixels.<br />
<img data-attachment-id="1395" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/superpixels/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/superpixels.jpg?fit=494%2C282&amp;ssl=1" data-orig-size="494,282" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="superpixels" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/superpixels.jpg?fit=450%2C257&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/superpixels.jpg?fit=450%2C257&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/superpixels.jpg?resize=450%2C257" alt="" class="size-medium wp-image-1395 aligncenter" width="450" height="257" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/superpixels.jpg?resize=450%2C257&amp;ssl=1 450w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/superpixels.jpg?w=494&amp;ssl=1 494w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />

The superpixels of can then be represented by groups of pixels all having the same pixel value, as shown here. This is what we will call <em>image segmentation</em>.<br />
<img data-attachment-id="1419" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/loresimage/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/loresimage.jpg?fit=450%2C256&amp;ssl=1" data-orig-size="450,256" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="loresimage" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/loresimage.jpg?fit=450%2C256&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/loresimage.jpg?fit=450%2C256&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/loresimage.jpg?resize=450%2C256" alt="" class="size-full wp-image-1419 aligncenter" width="450" height="256" data-recalc-dims="1" /><br />
<br />
In the next section we will introduce the concepts of superpixel image segmentation and the application of these packages. In Section 3 we will discuss the use of these concepts in data analysis. The intent of image segmentation as implemented in these and other software packages is to provide support for human visualization. This can cause problems because sometimes a feature that improves the process of visualization detracts from the purpose of data analysis. In Section 4 we will see how the simple structure of the objects created by Mouselimis’s two packages can be used to advantage to make data analysis more powerful and flexible. This post is a shortened version of the Additional Topic that I described at the start of the post.<br />
<br />
<em><strong>2. Introducing superpixel image segmentation<br />
</strong></em>The discussion in this section is adapted from the R vignette <a href="https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html"><em>Image Segmentation Based on Superpixels and Clustering</em></a> (Mouselimis, 2019b). This vignette describes the R packages <span style="font-family: courier new, courier, monospace">OpenImageR</span> and<span style="font-family: courier new, courier, monospace"> SuperpixelImageSegmentation</span>. In this post I will only discuss the <span style="font-family: courier new, courier, monospace">OpenImageR</span> package. It uses <em>simple linear iterative clustering</em> (SLIC, Achanta et al., 2010), and a modified version of this algorithm called SLICO (Yassine et al., 2018), and again for brevity I will only discuss the former. To understand how the SLIC algorithm works, and how it relates to our data analysis objective, we need a bit of review about the concept of color space. Humans perceive color as a mixture of the primary colors red, green, and blue (RGB), and thus a “model” of any given color can in principle be described as a vector in a three-dimensional vector space. The simplest such color space is one in which each component of the vector represents the intensity of one of the three primary colors. Landsat data used in land classification conforms to this model. Each band intensity is represented as an integer value between 0 and 255. The reason is that this covers the complete range of values that can be represented by a sequence of eight binary (0,1) values because 2<sup>8</sup> – 1 = 255.  For this reason it is called the eight-bit RGB color model.<br />
There are other color spaces beyond RGB, each representing a transformation for some particular purpose of the basic RGB color space. A common such color space is the CIELAB color space, defined by the International Commission of Illumination (CIE). You can read about this color space <a href="https://en.wikipedia.org/wiki/CIELAB_color_space">here</a>. In brief, quoting from this Wikipedia article, the CIELAB color space “expresses color as three values: L* for the lightness from black (0) to white (100), a* from green (−) to red (+), and b* from blue (−) to yellow (+). CIELAB was designed so that the same amount of numerical change in these values corresponds to roughly the same amount of visually perceived change.”<br />
Each pixel in an image contains two types of information: its color, represented by a vector in a three dimensional space such as the RGB or CIELAB color space, and its location, represented by a vector in two dimensional space (its <em>x</em> and <em>y</em> coordinates). Thus the totality of information in a pixel is a vector in a five dimensional space. The SLIC algorithm performs a clustering operation similar to <em>K-</em>means clustering on the collection of pixels as represented in this five-dimensional space.<br />
The SLIC algorithm measures total distance between pixels as the sum of two components, <em>d<sub>lab</sub></em><sub>, </sub>the distance in the CIELAB color space, and <em>d<sub>xy</sub></em>, the distance in pixel (<em>x</em>,<em>y</em>) coordinates. Specifically, the distance <em>D<sub>s</sub></em> is computed as<br />
<img data-attachment-id="1397" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/eq2/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/EQ2.jpg?fit=110%2C42&amp;ssl=1" data-orig-size="110,42" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="EQ2" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/EQ2.jpg?fit=110%2C42&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/EQ2.jpg?fit=110%2C42&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/EQ2.jpg?resize=139%2C53" alt="" class="alignnone  wp-image-1397 aligncenter" width="139" height="53" data-recalc-dims="1" />

where <em>S</em> is the grid interval of the pixels and <em>m</em> is a compactness parameter that allows one to emphasize or de-emphasize the spatial compactness of the superpixels. The algorithm begins with a set of <em>K</em> cluster centers regularly arranged in the (<em>x</em>,<em>y</em>) grid and performs <em>K</em>-means clustering of these centers until a predefined convergence measure is attained.<br />
The following discussion assumes some basic familiarity with the <span style="font-family: courier new, courier, monospace">raster</span> package. If you are not familiar with this package, an excellent introduction is given <a href="https://rspatial.org/raster/pkg/index.html">here</a>. After downloading the data files, you can use them to first construct <span style="font-family: courier new, courier, monospace">RasterLayer</span> objects and then combine them into a <span style="font-family: courier new, courier, monospace">RasterBrick</span>.

<p class="Listing"><span style="font-family: courier new, courier, monospace">library(raster)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; b2 &lt;- raster(&#8220;region_blue.grd&#8221;)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; b3 &lt;- raster(&#8220;region_green.grd&#8221;)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; b4 &lt;- raster(&#8220;region_red.grd&#8221;)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; b5 &lt;- raster(&#8220;region_IR.grd&#8221;)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; region.brick &lt;- brick(b2, b3, b4, b5)</span></p>

We will need to save the number of rows and columns for present and future use.<br />
<span style="font-family: courier new, courier, monospace">&gt; print(nrows &lt;- region.brick@nrows)</span><br />
<span style="font-family: courier new, courier, monospace"><strong>[1] 187<br />
</strong>&gt; print(ncols &lt;- region.brick@ncols)</span><br />
<span style="font-family: courier new, courier, monospace"><strong>[1] 329</strong></span><strong><br />
</strong>Next we plot the region.brick object (shown above) and write it to disk.<br />
<span style="font-family: courier new, courier, monospace">&gt; # False color image</span><br />
<span style="font-family: courier new, courier, monospace">&gt; plotRGB(region.brick, r = 4, g = 3, b = 2,<br />
+   stretch = &#8220;lin&#8221;)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; # Write the image to disk</span><br />
<span style="font-family: courier new, courier, monospace">&gt; jpeg(&#8220;FalseColor.jpg&#8221;, width = ncols,<br />
+   height = nrows)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; plotRGB(region.brick, r = 4, g = 3, b = 2,<br />
+   stretch = &#8220;lin&#8221;)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; dev.off()</span>

<br />
We will now use SLIC for image segmentation. The code here is adapted directly from the <a href="https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html">superpixels vignette</a>.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; library(OpenImageR)<br />
> False.Color &lt;- readImage(&#8220;FalseColor.jpg&#8221;)<br />
> Region.slic = superpixels(input_image =<br />
+   False.Color, method = &#8220;slic&#8221;, superpixel = 80,<br />
+   compactness = 30, return_slic_data = TRUE,<br />
+   return_labels = TRUE, write_slic = &#8220;&#8221;,<br />
+   verbose = FALSE)<br />
<strong>Warning message:<br />
The input data has values between 0.000000 and 1.000000. The image-data will be multiplied by the value: 255!<br />
</strong>&gt; imageShow(Region.slic$slic_data)</span>

<strong><br />
 </strong>We will discuss the warning message in Section 3. The function <span style="font-family: courier new, courier, monospace">imageShow()</span> is part of the <span style="font-family: courier new, courier, monospace">OpenImageR</span> package.<br />
The <span style="font-family: courier new, courier, monospace">OpenImageR</span> function <span style="font-family: courier new, courier, monospace">superpixels()</span> creates superpixels, but it does not actually create an image segmentation in the sense that we are using the term is defined in Section 1. This is done by functions in the <span style="font-family: courier new, courier, monospace">SuperPixelImageSegmentation</span> package (Mouselimis, 2018), whose discussion is also contained in the <a href="https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html">superpixels vignette</a>. This package is also described in the Additional Topic. As pointed out there, however, some aspects that make the function well suited for image visualization detract from its usefulness for data analysis, so we won’t discuss it further in this post. Instead, we will move directly to data analysis using the <span style="font-family: courier new, courier, monospace">OpenImageR</span> package.<br />
<br />
<strong><em>3. Data analysis using superpixels<br />
</em></strong>To use the functions in the <span style="font-family: courier new, courier, monospace">OpenImageR</span> package for data analysis, we must first understand the structure of objects created by these packages. The simple, open structure of the objects generated by the function <span style="font-family: courier new, courier, monospace">superpixels()</span> makes it an easy matter to use these objects for our purposes. Before we begin, however, we must discuss a preliminary matter: the difference between a <span style="font-family: courier new, courier, monospace">Raster*</span> object and a raster object. In Section 2 we used the function <span style="font-family: courier new, courier, monospace">raster()</span> to generate the blue, green, red and IR band objects b2, b3, b4, and b5, and then we used the function <span style="font-family: courier new, courier, monospace">brick()</span> to generate the object <span style="font-family: courier new, courier, monospace">region.brick</span>. Let’s look at the classes of these objects.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; class(b3)</span><br />
<span style="font-family: courier new, courier, monospace"><strong>[1] &#8220;RasterLayer&#8221;<br />
attr(,&#8221;package&#8221;)<br />
[1] &#8220;raster&#8221;<br />
</strong>&gt; class(full.brick)</span><br />
<span style="font-family: courier new, courier, monospace"><strong>[1] &#8220;RasterBrick&#8221;<br />
attr(,&#8221;package&#8221;)<br />
[1] &#8220;raster&#8221;</strong></span>

<br />
Objects created by the raster packages are called <span style="font-family: courier new, courier, monospace">Raster*</span> objects, with a capital “R”. This may seem like a trivial detail, but it is important to keep in mind because the objects generated by the functions in the <span style="font-family: courier new, courier, monospace">OpenImageR</span> and <span style="font-family: courier new, courier, monospace">SuperpixelImageSegmentation</span> packages are raster objects that are not <span style="font-family: courier new, courier, monospace">Raster*</span> objects (note the deliberate lack of a courier font for the word “raster”).<br />
In Section 2 we used the <span style="font-family: courier new, courier, monospace">OpenImageR</span> function<span style="font-family: courier new, courier, monospace"> readImage()</span> to read the image data into the object <span style="font-family: courier new, courier, monospace">False.Color</span> for analysis. Let’s take a look at the structure of this object.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; str(False.Color)</span><br />
<strong><span style="font-family: courier new, courier, monospace"> num [1:187, 1:329, 1:3] 0.373 0.416 0.341 0.204 0.165 &#8230;</span></strong>

<br />
It is a three-dimensional array, which can be thought of as a box, analogous to a rectangular Rubik’s cube. The “height” and “width” of the box are the number of rows and columns respectively in the image and at each value of the height and width the “depth” is a vector whose three components are the red, green, and blue color values of that cell in the image, scaled to [0,1] by dividing the 8-bit RGB values, which range from 0 to 255, by 255. This is the source of the warning message that accompanied the output of the function <span style="font-family: courier new, courier, monospace">superpixels()</span>, which said that the values will be multiplied by 255. For our purposes, however, it means that we can easily analyze images consisting of transformations of these values, such as the NDVI. Since the NDVI is a ratio, it doesn’t matter whether the RGB values are normalized or not. For our case the <span style="font-family: courier new, courier, monospace">RasterLayer</span> object <span style="font-family: courier new, courier, monospace">b5</span> of the <span style="font-family: courier new, courier, monospace">RasterBrick False.Color</span> is the <em>IR</em> band and the object <span style="font-family: courier new, courier, monospace">b4</span> is the <em>R</em> band. Therefore we can compute the NDVI as<br />
<br />


<p class="Listing"><span style="font-family: courier new, courier, monospace">&gt; NDVI.region &lt;- (b5 – b4) / (b5 + b4)</span></p>

Since NDVI is a ratio scale quantity, the theoretically best practice is to plot it using a monochromatic representation in which the brightness of the color (i.e., the color <em>value</em>) represents the value (Tufte, 1983). We can accomplish this using the <span style="font-family: courier new, courier, monospace">RColorBrewer</span> function <span style="font-family: courier new, courier, monospace">brewer.pal()</span>.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; library(RColorBrewer)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; plot(NDVI.region, col = brewer.pal(9, &#8220;Greens&#8221;),<br />
+   axes = TRUE,</span><span style="font-family: courier new, courier, monospace"> main = &#8220;Region NDVI&#8221;)<br />
 </span>

This generates the NDVI map shown above. The darkest areas have the highest NDVI. Let’s take a look at the structure of the <span style="font-family: courier new, courier, monospace">RasterLayer</span> object <span style="font-family: courier new, courier, monospace">NDVI.region</span>.<em></em><strong><em><br />
</em></strong>

<span style="font-family: courier new, courier, monospace">&gt; str(NDVI.region)</span><br />
<span style="font-family: courier new, courier, monospace"><strong>Formal class &#8216;RasterLayer&#8217; [package &#8220;raster&#8221;] with 12 slots<br />
              *                 *                   *<br />
..@ data    :Formal class &#8216;.SingleLayerData&#8217; [package &#8220;raster&#8221;] with 13 slots<br />
              *                 *                   *<br />
.. .. ..@ values    : num [1:61523] 0.1214 0.1138 0.1043 0.0973 0.0883 &#8230;<br />
              *                 *                   *<br />
..@ ncols   : int 329<br />
..@ nrows   : int 187<br />
</strong></span>

Only the relevant parts are shown, but we see that we can convert the raster data to a matrix that can be imported into our image segmentation machinery as follows. Remember that by default R constructs matrices by columns. The data in <span style="font-family: courier new, courier, monospace">Raster*</span> objects such as NDVI.region are stored by rows, so in converting these data to a matrix we must specify <span style="font-family: courier new, courier, monospace">byrow = TRUE</span>.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; NDVI.mat &lt;- matrix(NDVI.region@data@values,<br />
+   nrow = NDVI.region@nrows,<br />
+   </span><span style="font-family: courier new, courier, monospace">ncol = NDVI.region@ncols, byrow = TRUE)<br />
<br />
</span>

The function<span style="font-family: courier new, courier, monospace"> imageShow()</span> works with data that are either in the eight bit 0 – 255 range or in the [0,1] range (i.e., the range of <em>x</em> between and including 0 and 1). It does not, however, work with NDVI values if these values are negative. Therefore, we will scale NDVI values to [0,1].<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; m0 &lt;- min(NDVI.mat)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; m1 &lt;- max(NDVI.mat)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.mat1 &lt;- (NDVI.mat &#8211; m0) / (m1 &#8211; m0)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; imageShow(NDVI.mat)<br />
<br />
</span>

 Here is the resulting image.<strong><br />
<img data-attachment-id="1398" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/greyndvi/" data-orig-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/greyndvi.jpg?fit=495%2C282&amp;ssl=1" data-orig-size="495,282" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="greyndvi" data-image-description="" data-medium-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/greyndvi.jpg?fit=450%2C256&amp;ssl=1" data-large-file="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/greyndvi.jpg?fit=450%2C256&amp;ssl=1" loading="lazy" src="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/greyndvi.jpg?resize=450%2C256" alt="" class="size-medium wp-image-1398 aligncenter" width="450" height="256" srcset="https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/greyndvi.jpg?resize=450%2C256&amp;ssl=1 450w, https://i2.wp.com/r-posts.com/wp-content/uploads/2020/03/greyndvi.jpg?w=495&amp;ssl=1 495w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" /><br />
</strong>

The function <span style="font-family: courier new, courier, monospace">imageShow()</span> deals with a single layer object by producing a grayscale image. Unlike the green NDVI plot above, in this plot the lower NDVI regions are darker.<br />
We are now ready to carry out the segmentation of the NDVI data. Since the structure of the NDVI image to be analyzed is the same as that of the false color image, we can simply create a copy of this image, fill it with the NDVI data, and run it through the superpixel image segmentation function. If an image has not been created on disk, it is also possible (see the Additional Topic) to create the input object directly from the data.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; NDVI.data &lt;- False.Color</span><br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.data[,,1] &lt;- NDVI.mat1</span><br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.data[,,2] &lt;- NDVI.mat1</span><br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.data[,,3] &lt;- NDVI.mat1</span>

In the next section we will describe how to create an image segmentation of the NDVI data and how to use cluster analysis to create a land classification.<br />
<br />
<em><strong>4. Expanding the data analysis capacity of superpixels</strong></em><br />
Here is an application of the function <span style="font-family: courier new, courier, monospace">superpixels()</span> to the NDVI data generated in Section 3.<br />
<br />
<br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.80 = superpixels(input_image = NDVI.data,</span><br />
<span style="font-family: courier new, courier, monospace">+   method = &#8220;slic&#8221;, </span><span style="font-family: courier new, courier, monospace">superpixel = 80,</span><br />
<span style="font-family: courier new, courier, monospace">+   compactness = 30,</span><span style="font-family: courier new, courier, monospace"> return_slic_data = TRUE,</span><br />
<span style="font-family: courier new, courier, monospace">+   return_labels = TRUE, </span><span style="font-family: courier new, courier, monospace">write_slic = &#8220;&#8221;,</span><br />
<span style="font-family: courier new, courier, monospace">+   verbose = FALSE)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; imageShow(Region.slic$slic_data)<br />
<br />
</span>

Here is the result.<br />
<img data-attachment-id="1400" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/superndvi/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/superndvi.gif?fit=187%2C106&amp;ssl=1" data-orig-size="187,106" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="superndvi" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/superndvi.gif?fit=187%2C106&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/superndvi.gif?fit=187%2C106&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/superndvi.gif?resize=496%2C281" alt="" class="wp-image-1400 aligncenter" width="496" height="281" data-recalc-dims="1" />Here the structure of the object <span style="font-family: courier new, courier, monospace">NDVI.80</span> .<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; str(NDVI.80)</span><br />
<span style="font-family: courier new, courier, monospace"><strong>List of 2<br />
$ slic_data: num [1:187, 1:329, 1:3] 95 106 87 52 42 50 63 79 71 57 &#8230;<br />
$ labels   : num [1:187, 1:329] 0 0 0 0 0 0 0 0 0 0 &#8230;<br />
<br />
</strong></span>

It is a list with two elements, <span style="font-family: courier new, courier, monospace">NDVI.80$slic_data</span>, a three dimensional array of pixel color data (not normalized), and <span style="font-family: courier new, courier, monospace">NDVI.80$labels</span>, a matrix whose elements correspond to the pixels of the image. The second element’s name hints that it may contain values identifying the superpixel to which each pixel belongs. Let’s see if this is true.<br />
<br />
<span style="font-family: courier new, courier, monospace;font-size: 10pt">&gt; sort(unique(as.vector(NDVI.80$labels)))</span><br />
<span style="font-family: courier new, courier, monospace;font-size: 10pt"><strong>[1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23<br />
[25] 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47<br />
[49] 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71<br />
<br />
</strong></span>

There are 72 unique labels. Although the call to <span style="font-family: courier new, courier, monospace">superpixels()</span> specified 80 superpixels, the function generated 72. We can see which pixels have the label value 0 by setting the values of all of the other pixels to [255, 255, 255], which will plot as white.<br />
<br />
<span style="font-family: courier new, courier, monospace">&gt; R0 &lt;- NDVI.80</span><br />
<span style="font-family: courier new, courier, monospace">for (i in 1:nrow(R0$label))</span><br />
<span style="font-family: courier new, courier, monospace">+    for (j in 1:ncol(R0$label))</span><br />
<span style="font-family: courier new, courier, monospace">+       if (R0$label[i,j] != 0)</span><br />
<span style="font-family: courier new, courier, monospace">+          R0$slic_data[i,j,] &lt;- c(255,255,255)</span><br />
<span><span style="font-family: courier new, courier, monospace">&gt; imageShow(R0$slic_data)</span><br />
<br />
</span><br />
This produces the plot shown here.<br />
<img data-attachment-id="1402" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/corner/" data-orig-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/corner.jpg?fit=624%2C172&amp;ssl=1" data-orig-size="624,172" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="corner" data-image-description="" data-medium-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/corner.jpg?fit=450%2C124&amp;ssl=1" data-large-file="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/corner.jpg?fit=450%2C124&amp;ssl=1" loading="lazy" src="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/corner.jpg?resize=450%2C124" alt="" class="size-medium wp-image-1402 aligncenter" width="450" height="124" srcset="https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/corner.jpg?resize=450%2C124&amp;ssl=1 450w, https://i0.wp.com/r-posts.com/wp-content/uploads/2020/03/corner.jpg?w=624&amp;ssl=1 624w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />The operation isolates the superpixel in the upper right corner of the image, together with the corresponding portion of the boundary. We can easily use this approach to figure out which value of <span style="font-family: courier new, courier, monospace">NDVI.80$label</span> corresponds to which superpixel. Now let’s deal with the boundary. A little exploration of the <span style="font-family: courier new, courier, monospace">NDVI.80</span> object suggests that pixels on the boundary have all three components equal to zero. Let’s isolate and plot all such pixels by coloring all other pixels white.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; Bdry &lt;- NDVI.80</span><br />
<span style="font-family: courier new, courier, monospace">for (i in 1:nrow(Bdry$label))</span><br />
<span style="font-family: courier new, courier, monospace">+    for (j in 1:ncol(Bdry$label))</span><br />
<span style="font-family: courier new, courier, monospace">+     if (!(Bdry$slic_data[i,j,1] == 0 &amp;</span><br />
<span style="font-family: courier new, courier, monospace">+     Bdry$slic_data[i,j,2] == 0 &amp; <br />
+       Bdry$slic_data[i,j,3] == 0))</span><br />
<span style="font-family: courier new, courier, monospace">+       Bdry$slic_data[i,j,] &lt;- c(255,255,255)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; Bdry.norm &lt;- NormalizeObject(Bdry$slic_data)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; imageShow(Bdry$slic_data)<br />
<br />
</span>

<img data-attachment-id="1403" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/boundary/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/boundary.jpg?fit=304%2C173&amp;ssl=1" data-orig-size="304,173" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="boundary" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/boundary.jpg?fit=304%2C173&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/boundary.jpg?fit=304%2C173&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/boundary.jpg?resize=304%2C173" alt="" class="size-full wp-image-1403 aligncenter" width="304" height="173" data-recalc-dims="1" /><br />
<br />
This shows that we have indeed identified the boundary pixels. Note that the function <span style="font-family: courier new, courier, monospace">imageShow()</span> displays these pixels as white with a black edge, rather than pure black.

Having done a preliminary analysis, we can organize our segmentation process into two steps. The first step will be to replace each of the superpixels generated by the <span style="font-family: courier new, courier, monospace">OpenImageR</span> function <span style="font-family: courier new, courier, monospace">superpixels()</span> with one in which each pixel has the same value, corresponding to a measure of central tendency (e.g, the mean, median , or mode) of the original superpixel. The second step will be to use the <em>K</em>-means unsupervised clustering procedure to organize the superpixels from step 1 into a set of clusters and give each cluster a value corresponding to a measure of central tendency of the cluster. I used the code developed to identify the labels and to generate the  boundary to construct a function <span style="font-family: courier new, courier, monospace">make.segments()</span> to carry out the segmentation. The first argument of <span style="font-family: courier new, courier, monospace">make.segments()</span> is the superpixels object, and the second is the functional measurement of central tendency. Although in this case each of the three colors of the object <span style="font-family: courier new, courier, monospace">NDVI.80</span> have the same values, this may not be true for every application, so the function analyzes each color separately. Because the function is rather long, I have not included it in this post. If you want to see it, you can go to the Additional Topic linked to the book&#8217;s website.

<p style="text-align: left">Here is the application of the function to the object <span style="font-family: courier new, courier, monospace">NDVI.80</span> with the second argument set to <span style="font-family: courier new, courier, monospace">mean</span>.<br />
<br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.means &lt;- make.segments(NDVI.80, mean)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; imageShow(NDVI.means$slic_data) <br />
<br />
</span>Here is the result.<br />
<img data-attachment-id="1405" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/ndviseg/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/ndviseg.jpg?fit=324%2C184&amp;ssl=1" data-orig-size="324,184" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="ndviseg" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/ndviseg.jpg?fit=324%2C184&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/ndviseg.jpg?fit=324%2C184&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/ndviseg.jpg?resize=324%2C184" alt="" class="size-full wp-image-1405 aligncenter" width="324" height="184" data-recalc-dims="1" /><br />
The next step is to develop clusters that represent identifiable land cover types. In a real project the procedure would be to collect a set of ground truth data from the site, but that option is not available to us. Instead we will work with the true color rendition of the Landsat scene, shown here.<br />
<img data-attachment-id="1406" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/truecolor/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/truecolor.jpg?fit=467%2C266&amp;ssl=1" data-orig-size="467,266" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="truecolor" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/truecolor.jpg?fit=450%2C256&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/truecolor.jpg?fit=450%2C256&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/truecolor.jpg?resize=450%2C256" alt="" class="alignnone size-medium wp-image-1406 aligncenter" width="450" height="256" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/truecolor.jpg?resize=450%2C256&amp;ssl=1 450w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/truecolor.jpg?w=467&amp;ssl=1 467w" sizes="(max-width: 450px) 100vw, 450px" data-recalc-dims="1" />The land cover is subdivided using <em>K-</em>means into five types: <em>dense crop, medium crop, scrub, open</em>, and <em>water</em>.<br />
<br />
</p>

<span style="font-family: courier new, courier, monospace">&gt; set.seed(123)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.clus &lt;-<br />
+ kmeans(as.vector(NDVI.means$slic_data[,,1]), 5)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; vege.class &lt;- matrix(NDVI.clus$cluster,<br />
+    nrow = NDVI.region@nrows,</span><br />
<span style="font-family: courier new, courier, monospace">+    ncol = NDVI.region@ncols, byrow = FALSE)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; class.ras &lt;- raster(vege.class, xmn = W,</span><br />
<span style="font-family: courier new, courier, monospace">+    xmx = E, ymn = S, ymx = N, crs =</span><br />
<span style="font-family: courier new, courier, monospace">+    CRS(&#8220;+proj=utm +zone=10 +ellps=WGS84&#8221;))<br />
<br />
</span>

Next I used the <span style="font-family: courier new, courier, monospace">raster</span> function <span style="font-family: courier new, courier, monospace">ratify()</span> to assign descriptive factor levels to the clusters.<br />
<br />
<br />

<span style="font-family: courier new, courier, monospace">&gt; class.ras &lt;- ratify(class.ras)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; rat.class &lt;- levels(class.ras)[[1]]</span><br />
<span style="font-family: courier new, courier, monospace">&gt; rat.class$landcover &lt;- c(&#8220;Water&#8221;, &#8220;Open&#8221;,<br />
+  &#8220;Scrub&#8221;, &#8220;Med. Crop&#8221;, &#8220;Dense Crop&#8221;)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; levels(class.ras) &lt;- rat.class</span><br />
<span style="font-family: courier new, courier, monospace">&gt; levelplot(class.ras, margin=FALSE,<br />
+   col.regions= c(&#8220;blue&#8221;, &#8220;tan&#8221;,</span><br />
<span style="font-family: courier new, courier, monospace">+   &#8220;lightgreen&#8221;, &#8220;green&#8221;, &#8220;darkgreen&#8221;), <br />
+    main = &#8220;Land Cover Types&#8221;)<br />
<br />
<br />
</span>The result is shown in the figure at the start of the post. We can also overlay the original boundaries on top of the image. This is more easily done using <span style="font-family: courier new, courier, monospace">plot()</span> rather than <span style="font-family: courier new, courier, monospace">levelplot()</span>. The function <span style="font-family: courier new, courier, monospace">plot()</span> allows plots to be built up in a series of statements. The function <span style="font-family: courier new, courier, monospace">levelplot()</span> does not.<br />
<br />


<span style="font-family: courier new, courier, monospace">&gt; NDVI.rasmns &lt;- raster(NDVI.means$slic_data[,,1],<br />
+   xmn = W,</span><span style="font-family: courier new, courier, monospace"> xmx = E, ymn = S, ymx = N,<br />
+   crs = CRS(&#8220;+proj=utm +zone=10 +ellps=WGS84&#8221;))</span><br />
<span style="font-family: courier new, courier, monospace">&gt; NDVI.polymns &lt;- rasterToPolygons(NDVI.rasmns,<br />
+   dissolve = TRUE)</span><br />
<span style="font-family: courier new, courier, monospace">&gt; plot(class.ras, col = c(&#8220;blue&#8221;, &#8220;tan&#8221;,<br />
+   &#8220;lightgreen&#8221;, </span><span style="font-family: courier new, courier, monospace">&#8220;green&#8221;, &#8220;darkgreen&#8221;),<br />
+   main = &#8220;Land Cover Types&#8221;,</span><span style="font-family: courier new, courier, monospace"> legend = FALSE) </span><br />
<span style="font-family: courier new, courier, monospace">&gt; legend(&#8220;bottom&#8221;, legend = c(&#8220;Water&#8221;, &#8220;Open&#8221;,<br />
+   &#8220;Scrub&#8221;, &#8220;Med. Crop&#8221;,</span><span style="font-family: courier new, courier, monospace"> &#8220;Dense Crop&#8221;),<br />
+   fill = c(&#8220;blue&#8221;, &#8220;tan&#8221;, &#8220;lightgreen&#8221;, &#8220;green&#8221;,</span><br />
<span style="font-family: courier new, courier, monospace">+   &#8220;darkgreen&#8221;))</span><br />
<span style="font-family: courier new, courier, monospace">&gt; plot(NDVI.polymns, add = TRUE)<br />
<br />
</span>

<img data-attachment-id="1407" data-permalink="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/lastfig/" data-orig-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/lastfig.jpg?fit=384%2C384&amp;ssl=1" data-orig-size="384,384" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}" data-image-title="lastfig" data-image-description="" data-medium-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/lastfig.jpg?fit=300%2C300&amp;ssl=1" data-large-file="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/lastfig.jpg?fit=384%2C384&amp;ssl=1" loading="lazy" src="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/lastfig.jpg?resize=300%2C300" alt="" class="size-medium wp-image-1407 aligncenter" width="300" height="300" srcset="https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/lastfig.jpg?resize=300%2C300&amp;ssl=1 300w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/lastfig.jpg?resize=150%2C150&amp;ssl=1 150w, https://i1.wp.com/r-posts.com/wp-content/uploads/2020/03/lastfig.jpg?w=384&amp;ssl=1 384w" sizes="(max-width: 300px) 100vw, 300px" data-recalc-dims="1" /><br />

The application of image segmentation algorithms to remotely sensed image classification is a rapidly growing field, with numerous studies appearing every year. At this point, however, there is little in the way of theory on which to base an organization of the topic. If you are interested in following up on the subject, I encourage you to explore it on the Internet.<br />
<br />
<em><strong>References</strong></em><br />
Achanta, R., A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Susstrunk (2010). SLIC Superpixels. Ecole Polytechnique Fedrale de Lausanne Technical Report 149300.<br />
<br />


Appelhans, T., F. Detsch, C. Reudenbach and S. Woellauer (2017).  mapview: Interactive Viewing of Spatial Data in R. R package version 2.2.0.  <a href="https://CRAN.R-project.org/package=mapview">https://CRAN.R-project.org/package=mapview</a>

Bivand, R., E. Pebesma, and V. Gómez-Rubio. (2008). <em>Applied Spatial Data Analysis with R</em>. Springer, New York, NY.<br />
<br />


Frey, B.J. and D. Dueck (2006). Mixture modeling by affinity propagation. <em>Advances in Neural Information Processing Systems</em> 18:379.<br />
<br />


Hijmans, R. J. (2016). raster: Geographic Data Analysis and Modeling. R package version 2.5-8. <a href="https://CRAN.R-project.org/package=raster">https://CRAN.R-project.org/package=raster</a>

<br />
<br />
Mouselimis, L. (2018). SuperpixelImageSegmentation: Superpixel Image Segmentation. R package version 1.0.0. <a href="https://CRAN.R-project.org/package=SuperpixelImageSegmentation">https://CRAN.R-project.org/package=SuperpixelImageSegmentation</a>

<br />
<br />
Mouselimis, L. (2019a). OpenImageR: An Image Processing Toolkit. R package version 1.1.5. <a href="https://CRAN.R-project.org/package=OpenImageR">https://CRAN.R-project.org/package=OpenImageR</a>

<br />
<br />
Mouselimis, L. (2019b) <em>Image Segmentation Based on Superpixel Images and Clustering.</em> <a href="https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html">https://cran.r-project.org/web/packages/OpenImageR/vignettes/Image_segmentation_superpixels_clustering.html</a>.<br />
<br />


Ren, X., and J. Malik (2003) Learning a classification model for segmentation. International Conference on Computer Vision, 2003, 10-17.<br />
<br />


Stutz, D., A. Hermans, and B. Leibe (2018). Superpixels: An evaluation of the state-of-the-art. <em>Computer Vision and Image Understanding</em> 166:1-27.<br />
<br />


Tufte, E. R. (1983). <em>The Visual Display of Quantitative Information</em>. Graphics Press, Cheshire, Conn.<br />
<br />


Yassine, B., P. Taylor, and A. Story (2018).  Fully automated lung segmentation from chest radiographs using SLICO superpixels. Analog Integrated Circuits and Signal Processing 95:423-428.<br />
<br />


Zhou, B. (2013) Image segmentation using SLIC superpixels and affinity propagation clustering. International Journal of Science and Research. <a href="https://pdfs.semanticscholar.org/6533/654973054b742e725fd433265700c07b48a2.pdf">https://pdfs.semanticscholar.org/6533/654973054b742e725fd433265700c07b48a2.pdf</a>

&nbsp;

<p style="text-align: left"><br />
<br />
<br />
<br />
<br />
</p>

&lt;

p style=&#8221;text-align: center&#8221;><span style="font-family: courier new, courier, monospace"> </span><hr style="border-top:black solid 1px" /><a href="https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/">Analyzing Remote Sensing Data using Image Segmentation</a> was first posted on March 30, 2020 at 5:45 pm.<br />]]></content:encoded>
					
					<wfw:commentRss>https://r-posts.com/analyzing-remote-sensing-data-using-image-segmentation/feed/</wfw:commentRss>
			<slash:comments>2</slash:comments>
		
		
			</item>
	</channel>
</rss>
